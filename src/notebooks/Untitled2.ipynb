{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf8fc1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "import datasets\n",
    "import nltk  # Here to have a nice missing dependency error message early on\n",
    "import numpy as np\n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "import transformers\n",
    "from filelock import FileLock\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    HfArgumentParser,\n",
    "    MBart50Tokenizer,\n",
    "    MBart50TokenizerFast,\n",
    "    MBartTokenizer,\n",
    "    MBartTokenizerFast,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers.utils import check_min_version, is_offline_mode, send_example_telemetry\n",
    "from transformers.utils.versions import require_version\n",
    "\n",
    "\n",
    "from data import DataCollatorForSeq2SeqWithMultipleReferences\n",
    "from BSF_Trainer import BSFTrainer\n",
    "from trainer import CustomTrainer\n",
    "\n",
    "import traceback\n",
    "\n",
    "\n",
    "# Will error if the minimal version of Transformers is not installed. Remove at your own risks.\n",
    "# check_min_version(\"4.21.0.dev0\")\n",
    "\n",
    "# require_version(\"datasets>=1.8.0\", \"To fix: pip install -r examples/pytorch/summarization/requirements.txt\")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "try:\n",
    "    nltk.data.find(\"tokenizers/punkt\")\n",
    "except (LookupError, OSError):\n",
    "    if is_offline_mode():\n",
    "        raise LookupError(\n",
    "            \"Offline mode: run this script without TRANSFORMERS_OFFLINE first to download nltk data files\"\n",
    "        )\n",
    "    with FileLock(\".lock\") as lock:\n",
    "        nltk.download(\"punkt\", quiet=True)\n",
    "\n",
    "# A list of all multilingual tokenizer which require lang attribute.\n",
    "MULTILINGUAL_TOKENIZERS = [MBartTokenizer, MBartTokenizerFast, MBart50Tokenizer, MBart50TokenizerFast]\n",
    "\n",
    "import string\n",
    "import random\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import random    \n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "423940e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mohanad Alrefaai\\AppData\\Local\\Temp\\ipykernel_16852\\1199514906.py:12: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  rouge = load_metric(\"rouge\")\n"
     ]
    }
   ],
   "source": [
    "data_args = {\n",
    "    \"ner_mlm\": False,\n",
    "    \"ner_sgs_mlm\": True,\n",
    "    \"ner_mlm_prob\": 0.6\n",
    "}\n",
    "\n",
    "NER_MASK = \"<mask1>\"\n",
    "NER_TOKEN_MASK = \"<mask2>\"\n",
    "MLM_CONNECTOR = \"<conn1>\"\n",
    "MLM_SGS_CONNECTOR = \"<conn2>\"\n",
    "\n",
    "rouge = load_metric(\"rouge\")\n",
    "bertscore = load_metric('bertscore')\n",
    "\n",
    "try:\n",
    "    spacy_pipeline = spacy.load('en_core_web_sm')\n",
    "except OSError:\n",
    "    logging.warning(\"Downloading language model for the spaCy model.\")\n",
    "    from spacy.cli import download\n",
    "    download('en_core_web_sm')\n",
    "    spacy_pipeline = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "045e05b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "def extract_entties_with_spacy(source):\n",
    "    try:\n",
    "        spacy_pipeline = spacy.load('en_core_web_sm')\n",
    "    except OSError:\n",
    "        logging.warning(\"Downloading language model for the spaCy model.\")\n",
    "        from spacy.cli import download\n",
    "        download('en_core_web_sm')\n",
    "        spacy_pipeline = spacy.load('en_core_web_sm')\n",
    "\n",
    "    list_entities = []\n",
    "\n",
    "    spacy_doc = spacy_pipeline(source)\n",
    "    list_entities = [a.text for a in spacy_doc.ents]\n",
    "    \n",
    "    return list_entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc5733bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_scorer(sentence, text, entities):\n",
    "    text_else = text.replace(sentence, \"\", 1)\n",
    "    \n",
    "    # count entities\n",
    "    entites_count = 0\n",
    "    for entity in entities:\n",
    "        if entity in sentence:\n",
    "            entites_count += 1\n",
    "    entites_score = (entites_count * 1.0) / 4\n",
    "\n",
    "    # rouge score\n",
    "    result_rouge = rouge.compute(predictions=[sentence], references=[text], use_stemmer=True)\n",
    "    rouge_score = result_rouge.get(\"rouge1\").mid.fmeasure\n",
    "\n",
    "    # bertscore\n",
    "    bertscore_results = bertscore.compute(predictions=[sentence], references=[text], lang='en')\n",
    "    \n",
    "    bertscore_value = np.average(bertscore_results[\"f1\"])\n",
    "    \n",
    "    return entites_score + rouge_score + bertscore_value\n",
    "\n",
    "def mask_sentence(sentence, entities, prob_ner, prob_token):\n",
    "\n",
    "    tokens_sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "    processed = sentence\n",
    "    \n",
    "    \n",
    "    for entity in entities:\n",
    "        if entity in sentence:\n",
    "            tokens_sentence = tokens_sentence.replace(entity, \"\")\n",
    "            if random.random() <= prob_ner:\n",
    "                processed = sentence.replace(entity, NER_MASK)\n",
    "                \n",
    "    for token in tokens_sentence.split():\n",
    "        if token == NER_MASK:\n",
    "            continue\n",
    "        if random.random() <= prob_token:\n",
    "            processed = processed.replace(\" \" + token, \" \" + NER_TOKEN_MASK, 1)\n",
    "            processed = processed.replace(\" \" + token + \" \", \" \" + NER_TOKEN_MASK + \" \", 1)\n",
    "            processed = processed.replace(token + \" \", NER_TOKEN_MASK + \" \", 1)\n",
    "\n",
    "    for i in range(0, len(tokens_sentence.split())):\n",
    "        multiple_masks = NER_TOKEN_MASK\n",
    "        for j in range (0, i):\n",
    "            multiple_masks += \" \" + NER_TOKEN_MASK\n",
    "        processed = processed.replace(multiple_masks, NER_TOKEN_MASK)\n",
    "    \n",
    "    return processed\n",
    "    \n",
    "def custom_train_preprocess(examples):\n",
    "    \n",
    "  #  if (not data_args[\"ner_mlm\"]) and (not data_args[\"ner_sgs_mlm\"]):\n",
    "   #     return examples\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    list_entities = []\n",
    "    \n",
    "\n",
    "    documents = []\n",
    "    summaries = []\n",
    "    for i in range(len(examples[text_column])):\n",
    "\n",
    "        preprocessed_exp = examples[text_column][i]\n",
    "\n",
    "        list_entities = extract_entties_with_spacy(preprocessed_exp)\n",
    "        \n",
    "        sentences = nltk.sent_tokenize(preprocessed_exp)\n",
    "\n",
    "        examples_data = {\n",
    "            \"sentence\": sentences,\n",
    "        }\n",
    "        \n",
    "        df = pd.DataFrame(examples_data)\n",
    "        df[\"score\"] = df[\"sentence\"].map(lambda x: sentence_scorer(x, preprocessed_exp, list_entities))\n",
    "        df.sort_values(\"score\", ascending=False, inplace=True)\n",
    "        df[\"normalized_score\"] = df[\"score\"].map(lambda x: x / df[\"score\"].max())\n",
    "        df[\"masked\"] = df.apply(lambda x: mask_sentence(x[\"sentence\"], list_entities, data_args[\"ner_mlm_prob\"], x[\"normalized_score\"] * data_args[\"ner_mlm_prob\"]), axis=1)\n",
    "        df[\"num_masked1\"] = df[\"masked\"].map(lambda x: x.count(\"<mask1>\"))\n",
    "        df[\"num_masked2\"] = df[\"masked\"].map(lambda x: x.count(\"<mask2>\"))\n",
    "        df[\"length\"] = df[\"sentence\"].map(lambda x: len(x.split()))\n",
    "        df[\"length_masked\"] = df[\"masked\"].map(lambda x: len(x.split()))\n",
    "        df[\"masked_prob\"] = df.apply(lambda x: ((x[\"num_masked2\"] + x[\"num_masked1\"]) + (x[\"length\"] - x[\"length_masked\"])) / x[\"length\"], axis=1)\n",
    "        \n",
    "\n",
    "        if data_args[\"ner_mlm\"]:\n",
    "            # mlm for faithfull\n",
    "            summary = \" \".join(df[\"sentence\"].sort_index( ))\n",
    "        elif data_args[\"ner_sgs_mlm\"]:\n",
    "            # mlm-sgs for faithfull\n",
    "            summary = \" \".join(df.head(int(df.shape[0] / 3.0)).sort_index( )[\"sentence\"])\n",
    "            \n",
    "        preprocessed_exp = \" \".join(df[\"masked\"].sort_index())\n",
    "        if data_args[\"ner_mlm\"]:\n",
    "            preprocessed_exp = \" \".join([MLM_CONNECTOR, preprocessed_exp])\n",
    "        elif  data_args[\"ner_sgs_mlm\"]:\n",
    "            preprocessed_exp = \" \".join([MLM_CONNECTOR, MLM_SGS_CONNECTOR, preprocessed_exp])\n",
    "        \n",
    "                \n",
    "       # print(df.sort_index())\n",
    "       # print(df.describe())\n",
    "            \n",
    "        summaries.append(summary)\n",
    "        documents.append(preprocessed_exp)\n",
    "        \n",
    "    new_examples = {}\n",
    "    new_examples[text_column] = documents\n",
    "    new_examples[summary_column] = summaries\n",
    "    return new_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a82a2627",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "text_column = \"text\"\n",
    "summary_column = \"summary\"\n",
    "\n",
    "from datasets import load_from_disk\n",
    "\n",
    "wiki_datasets = load_from_disk(\"C:\\.cache\\huggingface\\datasets\\wiki_100_1000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c16c302",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wiki_datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[43mwiki_datasets\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m))\u001b[38;5;241m.\u001b[39mmap(\n\u001b[0;32m      2\u001b[0m     custom_train_preprocess,\n\u001b[0;32m      3\u001b[0m     batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      4\u001b[0m     num_proc\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;66;03m#data_args.preprocessing_num_workers,\u001b[39;00m\n\u001b[0;32m      5\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'wiki_datasets' is not defined"
     ]
    }
   ],
   "source": [
    "sample = wiki_datasets[\"train\"].select(range(10)).map(\n",
    "    custom_train_preprocess,\n",
    "    batched=True,\n",
    "    num_proc=1,#data_args.preprocessing_num_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10af090e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Hello How are you\"\n",
    "text = \"I am fine you?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ca0fc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_rouge = rouge.compute(predictions=[sentence], references=[text], use_stemmer=True, use_aggregator=False)\n",
    "rouge_score = result_rouge.get(\"rouge1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d78d70a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_score[0].fmeasure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79c23964",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The Academy Award for Best Production Design recognizes achievement for art direction in film. The category's original name was Best Art Direction, but was changed to its current name in 2012 for the 85th Academy Awards. This change resulted from the Art Director's branch of the Academy of Motion Picture Arts and Sciences (AMPAS) being renamed the Designer's branch. Since 1947, the award is shared with the set decorator(s). It is awarded to the best interior design in a film.\\n\\nThe films below are listed with their production year (for example, the 2000 Academy Award for Best Art Direction is given to a film from 1999). In the lists below, the winner of the award for each year is shown first, followed by the other nominees in alphabetical order.\\n\\nSuperlatives\\n\\nWinners and nominees\\n\\n1920s\\n\\n1930s\\n\\n1940s\\n\\n1950s\\n\\n1960s\\n\\n1970s\\n\\n1980s\\n\\n1990s\\n\\n2000s\\n\\n2010s\\n\\n2020s\\n\\nSee also\\n BAFTA Award for Best Production Design\\n Critics' Choice Movie Award for Best Production Design\\n\\nNotes\\n\\nReferences\\n\\nBest Production Design\\n\\nAwards for best art direction\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_datasets[\"train\"][\"text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21169995",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = wiki_datasets[\"train\"].select(range(10)).map(\n",
    "    custom_train_preprocess,\n",
    "    batched=True,\n",
    "    num_proc=2,#data_args.preprocessing_num_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "addda1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "def get_other_text(sentence, text):\n",
    "    res = text.replace(sentence, \"\", 1)\n",
    "    if (len(res) == 0 and len(res.split()) == 0):\n",
    "        res = text\n",
    "    return res\n",
    "def to_sentences(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    texts = [get_other_text(sent, text) for sent in sentences]\n",
    "\n",
    "    return {\n",
    "        \"text\": texts,\n",
    "        \"sentence\": sentences\n",
    "    }\n",
    "def preprocess(examples):\n",
    "    sentences = []\n",
    "    texts = []\n",
    "    for example in examples[\"text\"]:\n",
    "        res = to_sentences(example)\n",
    "        sentences.extend(res[\"sentence\"])\n",
    "        texts.extend(res[\"text\"])\n",
    "    return {\n",
    "        \"text\": texts,\n",
    "        \"sentence\": sentences\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8af01943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eed8824184f4c249a574927734e3772",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample = wiki_datasets[\"train\"].select(range(1000)).map(\n",
    "    preprocess,\n",
    "    batched=True,\n",
    "    remove_columns=wiki_datasets[\"train\"].column_names,\n",
    "    num_proc=1,#data_args.preprocessing_num_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9d9cbd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_score(examples):\n",
    "    #result_rouge = rouge.compute(predictions=examples[\"sentence\"], references=examples[\"text\"], use_stemmer=True, use_aggregator=False)\n",
    "    #rouge_score = [score.fmeasure for score in result_rouge.get(\"rouge1\")]\n",
    "    bertscore_results = bertscore.compute(predictions=examples[\"sentence\"], references=examples[\"text\"], lang='en')\n",
    "    bert_scores = bertscore_results[\"f1\"]\n",
    "    \n",
    "    #examples[\"rouge\"] = rouge_score\n",
    "    examples[\"bert_score\"] = bert_scores\n",
    "    return examples\n",
    "\n",
    "def pre_process__rouge_score(examples):\n",
    "    result_rouge = rouge.compute(predictions=examples[\"sentence\"], references=examples[\"text\"], use_stemmer=True, use_aggregator=False)\n",
    "    rouge_score = [score.fmeasure for score in result_rouge.get(\"rouge1\")]\n",
    "    #bertscore_results = bertscore.compute(predictions=examples[\"sentence\"], references=examples[\"text\"], lang='en')\n",
    "    #bert_scores = bertscore_results[\"f1\"]\n",
    "    \n",
    "    examples[\"rouge\"] = rouge_score\n",
    "    #examples[\"bert_score\"] = bert_scores\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c9720002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a21b1bb8b0a4ba482d0b6ed74945e55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20221 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_bert = sample.select(range(20221)).map(\n",
    "    pre_process_score,\n",
    "    batched=True,\n",
    "    num_proc=1,#data_args.preprocessing_num_workers,\n",
    "    batch_size=10000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8b708e6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b047bc85f3124189a484ad91fef05821",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_rouge = sample.select(range(1000)).map(\n",
    "    pre_process__rouge_score,\n",
    "    batched=True,\n",
    "    num_proc=1,#data_args.preprocessing_num_workers,\n",
    "    batch_size=10000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e8702a2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'sentence'],\n",
       "    num_rows: 20221\n",
       "})"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388e9d0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9620565531b45e6b212edac93e1f993",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=10):   0%|          | 0/20221 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def bert_map():\n",
    "    import string\n",
    "    import random\n",
    "\n",
    "\n",
    "    import pandas as pd\n",
    "    import spacy\n",
    "    import random    \n",
    "    import nltk\n",
    "    import math\n",
    "    from datasets import load_dataset, load_metric\n",
    "    import multiprocessing \n",
    "    import os\n",
    "    from multiprocessing import Process, Manager, Value\n",
    "    \n",
    "    \n",
    "    rouge = load_metric(\"rouge\")\n",
    "    def pre_process_score(examples):\n",
    "        #lock = multiprocessing.Lock()\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\" #\"{0}\".format( 0 if random.random()  <= 0.5 else 1)\n",
    "        \n",
    "        #os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"{0}\".format( 0 if random.random()  <= 0.5 else 1)\n",
    "        bertscore = load_metric('bertscore')\n",
    "        #result_rouge = rouge.compute(predictions=examples[\"sentence\"], references=examples[\"text\"], use_stemmer=True, use_aggregator=False)\n",
    "        #rouge_score = [score.fmeasure for score in result_rouge.get(\"rouge1\")]\n",
    "        bertscore_results = bertscore.compute(predictions=examples[\"sentence\"], references=examples[\"text\"], lang='en')\n",
    "        bert_scores = bertscore_results[\"f1\"]\n",
    "\n",
    "        #examples[\"rouge\"] = rouge_score\n",
    "        examples[\"bert_score\"] = bert_scores\n",
    "        return examples\n",
    "    \n",
    "    sample_bert = sample.select(range(20221)).map(\n",
    "        lambda x: pre_process_score(x),\n",
    "        batched=True,\n",
    "        num_proc=10,#data_args.preprocessing_num_workers,\n",
    "        batch_size=500\n",
    "    )\n",
    "\n",
    "bert_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "b0b094b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'sentence'],\n",
       "    num_rows: 20221\n",
       "})"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6992ec77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_wiki(num_proc = 1, selections = 100, batch_size = 1000):\n",
    "    import string\n",
    "    import random\n",
    "\n",
    "\n",
    "    import pandas as pd\n",
    "    import spacy\n",
    "    import random    \n",
    "    import nltk\n",
    "    \n",
    "    import evaluate\n",
    "    \n",
    "    from datasets import load_dataset, load_metric\n",
    "    data_args = {\n",
    "        \"ner_mlm\": False,\n",
    "        \"ner_sgs_mlm\": True,\n",
    "        \"ner_mlm_prob\": 0.3\n",
    "    }\n",
    "\n",
    "    NER_MASK = \"<mask1>\"\n",
    "    NER_TOKEN_MASK = \"<mask2>\"\n",
    "    MLM_CONNECTOR = \"<conn1>\"\n",
    "    MLM_SGS_CONNECTOR = \"<conn2>\"\n",
    "    \n",
    "    text_column = \"text\"\n",
    "    summary_column = \"summary\"\n",
    "\n",
    "    #bertscore = load_metric('bertscore')\n",
    "\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    \n",
    "    try:\n",
    "        spacy_pipeline = spacy.load('en_core_web_sm', enable=[\"ner\"])\n",
    "    except OSError:\n",
    "        logging.warning(\"Downloading language model for the spaCy model.\")\n",
    "        from spacy.cli import download\n",
    "        download('en_core_web_sm')\n",
    "        spacy_pipeline = spacy.load('en_core_web_sm')\n",
    "\n",
    "        \n",
    "    def get_other_text(sentence, text):\n",
    "        res = text.replace(sentence, \"\", 1)\n",
    "        if (len(res) == 0 and len(res.split()) == 0):\n",
    "            res = text\n",
    "        return res\n",
    "\n",
    "    def extract_entties_with_spacy(source, spacy_pipeline):\n",
    "        \n",
    "        list_entities = []\n",
    "\n",
    "        spacy_doc = spacy_pipeline(source)#, n_process=-1)\n",
    "        list_entities = [a.text for a in spacy_doc.ents]\n",
    "\n",
    "        return list_entities\n",
    "\n",
    "    \n",
    "    def sentence_scorer(sentence, text, entities, rouge_score):\n",
    "\n",
    "        # count entities\n",
    "        entites_count = 0\n",
    "        for entity in entities:\n",
    "            if entity in sentence:\n",
    "                entites_count += 1\n",
    "        entites_score = (entites_count * 1.0) / 4\n",
    "\n",
    "        # bertscore\n",
    "        #bertscore_results = bertscore.compute(predictions=[sentence], references=[text], lang='en')\n",
    "\n",
    "#        bertscore_value = np.average(bertscore_results[\"f1\"])\n",
    "\n",
    "        return entites_score + rouge_score# + bertscore_value\n",
    "\n",
    "    def mask_sentence(sentence, entities, prob_ner, prob_token):\n",
    "\n",
    "        tokens_sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "        processed = sentence\n",
    "\n",
    "\n",
    "        for entity in entities:\n",
    "            if entity in sentence:\n",
    "                tokens_sentence = tokens_sentence.replace(entity, \"\")\n",
    "                if random.random() <= prob_ner:\n",
    "                    processed = sentence.replace(entity, NER_MASK)\n",
    "\n",
    "        tokens_sentnece_list = tokens_sentence.split()\n",
    "        for token in tokens_sentnece_list:\n",
    "            if token == NER_MASK:\n",
    "                continue\n",
    "            if random.random() <= prob_token:\n",
    "                processed = processed.replace(\" \" + token, \" \" + NER_TOKEN_MASK, 1)\n",
    "                processed = processed.replace(\" \" + token + \" \", \" \" + NER_TOKEN_MASK + \" \", 1)\n",
    "                processed = processed.replace(token + \" \", NER_TOKEN_MASK + \" \", 1)\n",
    "\n",
    "        for i in range(0, len(tokens_sentnece_list)):\n",
    "            multiple_masks = NER_TOKEN_MASK\n",
    "            for j in range (0, i):\n",
    "                multiple_masks += \" \" + NER_TOKEN_MASK\n",
    "            processed = processed.replace(multiple_masks, NER_TOKEN_MASK)\n",
    "\n",
    "        return processed\n",
    "    \n",
    "    def to_sentences(text):\n",
    "        sentences = nltk.sent_tokenize(text)\n",
    "        texts = [get_other_text(sent, text) for sent in sentences]\n",
    "        \n",
    "        return (texts, sentences)\n",
    "    \n",
    "    def custom_train_preprocess(examples):\n",
    "    \n",
    "        #batch_texts = \". \".join(examples[text_column])\n",
    "        \n",
    "        \n",
    "        list_entities = []\n",
    "\n",
    "        #list_entities = extract_entties_with_spacy(batch_texts, spacy_pipeline)\n",
    "\n",
    "        all_texts = []\n",
    "        all_sentences = []\n",
    "        groups = []\n",
    "        for i in range(len(examples[text_column])):\n",
    "\n",
    "            preprocessed_exp = examples[text_column][i]\n",
    "\n",
    "        \n",
    "            texts, sentences = to_sentences(preprocessed_exp)\n",
    "            start_index = len(all_texts)\n",
    "            for tex in texts:\n",
    "                all_texts.append(tex)\n",
    "            for sen in sentences:\n",
    "                all_sentences.append(sen)\n",
    "#            all_texts.extend(texts)\n",
    " #           all_sentences.extend(sentences)\n",
    "            end_index = len(all_texts)\n",
    "            groups.append((start_index, end_index, sentences))\n",
    "\n",
    "        # compute rouge score for the whole batch\n",
    "    \n",
    "        result_rouge = rouge.compute(predictions=all_sentences, references=all_texts, rouge_types=[\"rouge1\"], use_aggregator=False)\n",
    "        rouge_scores = [score for score in result_rouge.get(\"rouge1\")]\n",
    "        #rouge_scores = [random.random() for sent in all_sentences]\n",
    "        \n",
    "            \n",
    "        documents = []\n",
    "        summaries = []\n",
    "        for i in range(len(examples[text_column])):\n",
    "\n",
    "            preprocessed_exp = examples[text_column][i]\n",
    "\n",
    "            list_entities = extract_entties_with_spacy(preprocessed_exp, spacy_pipeline)\n",
    "            \n",
    "            \n",
    "          #  texts, sentences = to_sentences(preprocessed_exp)\n",
    "\n",
    "            sentences = groups[i][2]\n",
    "            this_rouge_scores = rouge_scores[groups[i][0]:groups[i][1]]\n",
    "            \n",
    "         #   result_rouge = rouge.compute(predictions=sentences, references=texts, use_stemmer=True, use_aggregator=False)\n",
    "          #  rouge_scores = [score.fmeasure for score in result_rouge.get(\"rouge1\")]\n",
    "            \n",
    "            examples_data = {\n",
    "                \"sentence\": sentences,\n",
    "           #     \"rouge1\": rouge_scores\n",
    "                \"rouge1\": this_rouge_scores\n",
    "            }\n",
    "\n",
    "            df = pd.DataFrame(examples_data)\n",
    "            df[\"score\"] = df.apply(lambda x: sentence_scorer(x[\"sentence\"], preprocessed_exp, list_entities, x[\"rouge1\"]), axis=1)\n",
    "            df.sort_values(\"score\", ascending=False, inplace=True)\n",
    "            df[\"normalized_score\"] = df[\"score\"].map(lambda x: x / (df[\"score\"].max() + 0.001))\n",
    "            df[\"masked\"] = df.apply(lambda x: mask_sentence(x[\"sentence\"], list_entities, data_args[\"ner_mlm_prob\"], x[\"normalized_score\"] * data_args[\"ner_mlm_prob\"]), axis=1)\n",
    "            #df[\"num_masked1\"] = df[\"masked\"].map(lambda x: x.count(\"<mask1>\"))\n",
    "            #df[\"num_masked2\"] = df[\"masked\"].map(lambda x: x.count(\"<mask2>\"))\n",
    "            #df[\"length\"] = df[\"sentence\"].map(lambda x: len(x.split()))\n",
    "            #df[\"length_masked\"] = df[\"masked\"].map(lambda x: len(x.split()))\n",
    "            #df[\"masked_prob\"] = df.apply(lambda x: ((x[\"num_masked2\"] + x[\"num_masked1\"]) + (x[\"length\"] - x[\"length_masked\"])) / x[\"length\"], axis=1)\n",
    "\n",
    "\n",
    "            #print(df)\n",
    "            \n",
    "            if data_args[\"ner_mlm\"]:\n",
    "                # mlm for faithfull\n",
    "                summary = \" \".join(df[\"sentence\"].sort_index( ))\n",
    "            elif data_args[\"ner_sgs_mlm\"]:\n",
    "                # mlm-sgs for faithfull\n",
    "                summary = \" \".join(df.head(int(df.shape[0] / 3.0)).sort_index( )[\"sentence\"])\n",
    "\n",
    "            preprocessed_exp = \" \".join(df[\"masked\"].sort_index())\n",
    "            if data_args[\"ner_mlm\"]:\n",
    "                preprocessed_exp = \" \".join([MLM_CONNECTOR, preprocessed_exp])\n",
    "            elif  data_args[\"ner_sgs_mlm\"]:\n",
    "                preprocessed_exp = \" \".join([MLM_CONNECTOR, MLM_SGS_CONNECTOR, preprocessed_exp])\n",
    "\n",
    "\n",
    "           # print(df.sort_index())\n",
    "           # print(df.describe())\n",
    "\n",
    "            summaries.append(summary)\n",
    "            documents.append(preprocessed_exp)\n",
    "\n",
    "        new_examples = {}\n",
    "        new_examples[text_column] = documents\n",
    "        new_examples[summary_column] = summaries\n",
    "        return new_examples\n",
    "    \n",
    "    return wiki_datasets[\"train\"].select(range(selections)).map(\n",
    "        custom_train_preprocess,\n",
    "        batched=True,\n",
    "        num_proc=num_proc,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2120942d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f59082c751824d579fca688bd31d1249",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=10):   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample = pre_process_wiki(num_proc=10, selections=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1627fa7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04339697bede4e38afdfe82ff50d5e7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'url', 'title', 'text', 'summary'],\n",
       "    num_rows: 20000\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_process_wiki(num_proc=16, selections=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aaa274ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf1b7684bcf741df998581d97087d369",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=24):   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sampple2000 = pre_process_wiki(num_proc=24, selections=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d863047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63d6b74ec83d4931a3ca8b2b3dfd2878",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=24):   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sampple2000 = pre_process_wiki(num_proc=24, selections=2000, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c25b07a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e75775afbe8840fcb10522909beb0788",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=24):   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sampple2000 = pre_process_wiki(num_proc=24, selections=2000, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a79cef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eba1bf0c",
   "metadata": {},
   "source": [
    "sampple2000 = pre_process_wiki(num_proc=20, selections=2000, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3956c34d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97852d39a2d64c719bce3e53ea079a25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=20):   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sampple2000 = pre_process_wiki(num_proc=20, selections=2000, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2621d8f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ba0aceaa5a44c32bd9193dd2115252b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sampple2000 = pre_process_wiki(num_proc=2, selections=2000, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ab9654f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f6127383c154e48af64dfd71f687592",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/4000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sampple2000 = pre_process_wiki(num_proc=8, selections=4000, batch_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "62916217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7efe9ff516c24b3a8f5b2eb362a03786",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/4000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sampple2000 = pre_process_wiki(num_proc=16, selections=4000, batch_size=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8dc3a4ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24a8086f94214fb9904153b89a22dab9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sampple2000 = pre_process_wiki(num_proc=4, selections=4000, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "396ac99a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c470266f7ba4d47b86f1f467d80b555",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/4000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sampple2000 = pre_process_wiki(num_proc=8, selections=4000, batch_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "070153b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6c01f87db3c49129c067945d2b4eb72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/4000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sampple2000 = pre_process_wiki(num_proc=16, selections=4000, batch_size=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7a1de760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24fdbc1dc42645229cadcfd5d8915f31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/4000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sampple2000 = pre_process_wiki(num_proc=16, selections=4000, batch_size=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "05664540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d85773722ffd4c5dbe962796673d5534",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/4000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sampple2000 = pre_process_wiki(num_proc=16, selections=4000, batch_size=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "af742288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55b968c4f55541748ee1ba578d8d00ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/4000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sampple2000 = pre_process_wiki(num_proc=16, selections=4000, batch_size=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c26e0a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampple2000 = pre_process_wiki(num_proc=16, selections=4000, batch_size=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5bce3260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fd47e25f0d54799bce4df0d794de594",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=10):   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sampple2000 = pre_process_wiki(num_proc=10, selections=20000, batch_size=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b9d785b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'url', 'title', 'text'],\n",
       "        num_rows: 3767787\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1dba2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpu_spacy_process(num_proc = 1, ranges = range(100), batch_size = 1000):\n",
    "    \n",
    "    from thinc.api import set_gpu_allocator, require_gpu\n",
    "\n",
    "    import spacy\n",
    "    def process(examples):\n",
    "\n",
    "        require_gpu(0)\n",
    "        # Use the GPU, with memory allocations directed via PyTorch.\n",
    "        # This prevents out-of-memory errors that would otherwise occur from competing\n",
    "        # memory pools.\n",
    "        #set_gpu_allocator(\"pytorch\")\n",
    "        #require_gpu(1 if random.random() < 0.3 else 0)\n",
    "        try:\n",
    "            spacy_pipeline = spacy.load('en_core_web_sm', enable=[\"ner\"])\n",
    "        except OSError:\n",
    "            logging.warning(\"Downloading language model for the spaCy model.\")\n",
    "            from spacy.cli import download\n",
    "            download('en_core_web_sm')\n",
    "            spacy_pipeline = spacy.load('en_core_web_sm')\n",
    "            \n",
    "        texts = examples[\"text\"]\n",
    "        list_entities = []\n",
    "\n",
    "        spacy_docs = spacy_pipeline.pipe(texts)\n",
    "        for doc in spacy_docs:\n",
    "            list_entities.append([a.text for a in doc.ents])\n",
    "\n",
    "        examples[\"ner\"] = list_entities\n",
    "        return examples\n",
    "\n",
    "    return wiki_datasets[\"train\"].select(ranges).map(\n",
    "        process,\n",
    "        batched=True,\n",
    "        num_proc=num_proc,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "#gpu_spacy_process(2, 20000, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5dcd3022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aad94a1647f4b9eb4a956892b95f88f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_ner = gpu_spacy_process(8, 20000, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7c3c450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47de59a1c1c241b1b1794e8750902d2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_ner = gpu_spacy_process(8, 20000, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46666bfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d241bccfb9a04d7f8355eb108b9ba32e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_ner = gpu_spacy_process(20, 20000, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68d7b6ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fec2d701a154abca259f74518b764aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=20):   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_ner = gpu_spacy_process(20, 20000, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64d0a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_proc must be <= 10. Reducing num_proc to 10 for dataset of size 10.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d214f9d944aa4b239cea918955026771",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=10):   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(0, 20):\n",
    "    step_size = 10\n",
    "    start_index = i*step_size\n",
    "    end_index = (i+1)*step_size\n",
    "    if end_index > wiki_datasets[\"train\"].shape[0]:\n",
    "        end_index =  wiki_datasets[\"train\"].shape[0]\n",
    "    r = range(start_index, end_index)\n",
    "    sample_ner = gpu_spacy_process(20, r, 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e72ca9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "188389.35"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_datasets[\"train\"].shape[0] / 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7b4273b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b30921cb575f43a98f0eb2c1b804740c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=10):   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_ner = gpu_spacy_process(10, range(20000), 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1e3415e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f51a223acc104a63afc84eaad1b8a067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=10):   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_ner = gpu_spacy_process(10, range(20000), 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "014f72fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2920cb6ed814d21a862b34192e2b978",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=10):   0%|          | 0/200000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_ner = gpu_spacy_process(10, range(200000), 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3bd36536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ca538a8107c4dea8a113420417ea26a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_ner = gpu_spacy_process(1, range(20000), 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f691e18f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6050abd565443b29730659e71e27877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=10):   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_ner = gpu_spacy_process(10, range(20000), 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "51a2b9f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8691205dec044cfca4342487d1cc1b2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=10):   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_ner = gpu_spacy_process(10, range(20000), 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aad88693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "407b91a2cef540969005fd29dcc11420",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=10):   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_ner = gpu_spacy_process(10, range(20000), 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bae2c01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2ee8375a1cf41b3a1fb99dc514de3cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=10):   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_ner = gpu_spacy_process(10, range(20000), 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb776b4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eb78a5ebcd140409b4a323b7c005ba6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=10):   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_ner = gpu_spacy_process(10, range(20000), 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cce09125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f699d57b22404dfb95d50296c4918c8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=10):   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_ner = gpu_spacy_process(10, range(20000), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd3770e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bba3ba0d918c443ca74a4c39922555fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=10):   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_ner = gpu_spacy_process(10, range(20000), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "147d3c35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da37188619bd4293b252f94175876c14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=10):   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_ner = gpu_spacy_process(10, range(20000), 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1548ae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad1f5270570446bc86ee771668f6cb75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=10):   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_ner = gpu_spacy_process(10, range(20000), 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ac66cf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f352ece6a91147d4a8969208c717b101",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=10):   0%|          | 0/40000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_ner = gpu_spacy_process(10, range(40000), 4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7fc95b3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5d58c1d80fa48ffbdc1b06743d47f16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=12):   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_ner = gpu_spacy_process(12, range(20000), 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17c53bcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e0910186d0d4aa3aaa159b5309c141c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=12):   0%|          | 0/40000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_ner = gpu_spacy_process(12, range(40000), 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f06ab0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6180e4b2ac924ddf9dca6b4c70b4cd2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=10):   0%|          | 0/40000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_ner = gpu_spacy_process(10, range(40000), 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8410e676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1333aa3b04f74b85b459d487584d4398",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=10):   0%|          | 0/40000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_ner = gpu_spacy_process(10, range(40000), 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e999c9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0943de97e3664fa4b6b2f77e100670eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=10):   0%|          | 0/40000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_ner = gpu_spacy_process(10, range(40000), 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b845f2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "982e85aa8f4f4e9f843b0e6e5ecc7f3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=10):   0%|          | 0/40000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_ner = gpu_spacy_process(10, range(40000), 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0e978351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e04febee204548628be448f9d8f0a76f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=10):   0%|          | 0/40000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_ner = gpu_spacy_process(10, range(40000), 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1aa3012c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69720e4bbf6c4ed6af870c63a36590f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=10):   0%|          | 0/40000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_ner = gpu_spacy_process(10, range(40000), 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e1430ee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5716ab407484b3bb1b6cbda30a207ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=10):   0%|          | 0/40000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_ner = gpu_spacy_process(10, range(40000), 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b7e7db44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1247943f2f8246f585336e5d8556260f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=10):   0%|          | 0/40000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_ner = gpu_spacy_process(10, range(40000), 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "07993768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c88b72fac2814372bae7c42180ca28b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=12):   0%|          | 0/40000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_ner = gpu_spacy_process(12, range(40000), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "56290c34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7c15f00bd87442a810e2a7228488387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=12):   0%|          | 0/40000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_ner = gpu_spacy_process(12, range(40000), 225)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72a6cc56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee97ac1b968b4166a8a2053c33bb878b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=10):   0%|          | 0/40000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_ner = gpu_spacy_process(10, range(40000), 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e067270",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e02df3ff5114f369a56bb1390a4df3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=10):   0%|          | 0/40000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_ner = gpu_spacy_process(10, range(40000), 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3e419e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef03412b7682476d8d5214e392dc66d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=12):   0%|          | 0/40000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_ner = gpu_spacy_process(12, range(40000), 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6729bd89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34c132ad3e6f4ce29a2fa4b7a019bae7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=11):   0%|          | 0/40000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_ner = gpu_spacy_process(10, range(40000), 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6e6cdac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c848b37ac365439c8ddcb77ac063a037",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=10):   0%|          | 0/40000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_ner = gpu_spacy_process(10, range(40000), 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ecef3d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf4f210ad115416c93ced39a44008f58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=10):   0%|          | 0/40000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_ner = gpu_spacy_process(10, range(40000), 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78244c76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed74c84ccb904eb9b3499605e1c4284a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=10):   0%|          | 0/40000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_ner = gpu_spacy_process(10, range(40000), 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "61228680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4442ed6656294ae88df4a247bd4dbe5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=10):   0%|          | 0/40000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_ner = gpu_spacy_process(10, range(40000), 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "50bead9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80c0b572cf9b4e6993746f05b1d5e027",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/40000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_ner = gpu_spacy_process(8, range(40000), 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "672fce87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2470e4a7950430191cb93cb14cf81ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=6):   0%|          | 0/40000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_ner = gpu_spacy_process(6, range(40000), 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5fc3d624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03d055f0c48f4fa287d00529746ccb00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/40000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_ner = gpu_spacy_process(8, range(40000), 350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de7b58e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3001a8c24ed54b3c90c4c2cfdb9af411",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/40000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_ner = gpu_spacy_process(8, range(40000), 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5eaf6815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40c60fbf62624c71bd29a6a75a699747",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=9):   0%|          | 0/40000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_ner = gpu_spacy_process(9, range(40000), 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "977ab479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b5fe313dd2e41b09e66bbe12ffdacaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=10):   0%|          | 0/40000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_ner = gpu_spacy_process(10, range(40000), 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d58f8015",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "430abfb0c8bb4ddab353d65a04da38c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=9):   0%|          | 0/40000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_ner = gpu_spacy_process(9, range(40000), 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a0bb33ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3da3aa13842340b2b5f1e680a7476d7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=9):   0%|          | 0/40000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_ner = gpu_spacy_process(9, range(40000), 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "513238c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc57752381444964994e7b9cb0f94a8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=9):   0%|          | 0/3767787 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_ner = gpu_spacy_process(9, range(wiki_datasets[\"train\"].shape[0]), 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ce78c0c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'url', 'title', 'text', 'ner'],\n",
       "    num_rows: 3767787\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0e376be7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c46a816eedd423f83853e05e2953fe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/25 shards):   0%|          | 0/3767787 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_ner.save_to_disk(\"C:\\.cache\\huggingface\\datasets\\wiki_ner_100_1000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7aa62262",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ds(dataset, output_dir, num_proc = 1, ranges = range(100), batch_size = 1000):\n",
    "    \n",
    "    def gpu_spacy_process(dataset, ranges = range(100), batch_size = 1000):\n",
    "\n",
    "        from thinc.api import set_gpu_allocator, require_gpu\n",
    "\n",
    "        import spacy\n",
    "        def process(examples):\n",
    "\n",
    "            require_gpu(0)\n",
    "            # Use the GPU, with memory allocations directed via PyTorch.\n",
    "            # This prevents out-of-memory errors that would otherwise occur from competing\n",
    "            # memory pools.\n",
    "            #set_gpu_allocator(\"pytorch\")\n",
    "            #require_gpu(1 if random.random() < 0.3 else 0)\n",
    "            try:\n",
    "                spacy_pipeline = spacy.load('en_core_web_sm', enable=[\"ner\"])\n",
    "            except OSError:\n",
    "                logging.warning(\"Downloading language model for the spaCy model.\")\n",
    "                from spacy.cli import download\n",
    "                download('en_core_web_sm')\n",
    "                spacy_pipeline = spacy.load('en_core_web_sm')\n",
    "\n",
    "            texts = examples[\"article\"]\n",
    "            list_entities = []\n",
    "\n",
    "            spacy_docs = spacy_pipeline.pipe(texts)\n",
    "            for doc in spacy_docs:\n",
    "                list_entities.append([a.text for a in doc.ents])\n",
    "\n",
    "            examples[\"ner\"] = list_entities\n",
    "            return examples\n",
    "\n",
    "        return dataset.select(ranges).map(\n",
    "            process,\n",
    "            batched=True,\n",
    "            num_proc=1,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "    \n",
    "    def pre_process_dataset(dataset, num_proc = 1, ranges = range(100), batch_size = 1000):\n",
    "        import string\n",
    "        import random\n",
    "\n",
    "\n",
    "        import pandas as pd\n",
    "        import random    \n",
    "        import nltk\n",
    "        import math\n",
    "\n",
    "        import evaluate\n",
    "        import numpy as np\n",
    "\n",
    "        from datasets import load_dataset, load_metric\n",
    "        data_args = {\n",
    "            \"ner_mlm\": True,\n",
    "            \"ner_sgs_mlm\": False,\n",
    "            \"ner_mlm_prob\": 0.6\n",
    "        }\n",
    "\n",
    "        NER_MASK = \"<mask1>\"\n",
    "        NER_TOKEN_MASK = \"<mask2>\"\n",
    "        MLM_CONNECTOR = \"<conn1>\"\n",
    "        MLM_SGS_CONNECTOR = \"<conn2>\"\n",
    "\n",
    "        text_column = \"article\"\n",
    "        summary_column = \"abstract\"\n",
    "\n",
    "        #bertscore = load_metric('bertscore')\n",
    "\n",
    "        rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "\n",
    "        def get_other_text(sentence, text):\n",
    "            res = text.replace(sentence, \"\", 1)\n",
    "            if (len(res) == 0 and len(res.split()) == 0):\n",
    "                res = text\n",
    "            return res\n",
    "\n",
    "        def extract_entties_with_spacy(source, spacy_pipeline):\n",
    "\n",
    "            list_entities = []\n",
    "\n",
    "            spacy_doc = spacy_pipeline(source)#, n_process=-1)\n",
    "            list_entities = [a.text for a in spacy_doc.ents]\n",
    "\n",
    "            return list_entities\n",
    "\n",
    "\n",
    "        def mask_sentence(sentence, entities, prob_ner, prob_token):\n",
    "\n",
    "            tokens_sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "            processed = sentence\n",
    "            tokens_sentnece_list = tokens_sentence.split()\n",
    "\n",
    "            tokens_sentnece_list_masking = np.zeros(len(tokens_sentnece_list))\n",
    "\n",
    "            for entity in entities:\n",
    "                if entity in sentence:\n",
    "                    tokens_sentence = tokens_sentence.replace(entity, \"\")\n",
    "                    if random.random() <= prob_ner:\n",
    "                        entity_tokens = entity.split()\n",
    "                        entity_length = len(entity_tokens)\n",
    "                        for tokens_index in range(len(tokens_sentnece_list)):\n",
    "                            tokens_seq = \" \".join(tokens_sentnece_list[tokens_index:tokens_index + entity_length])\n",
    "                            entity_seq = entity\n",
    "                            if tokens_seq == entity_seq:\n",
    "                                tokens_sentnece_list_masking[tokens_index:tokens_index + entity_length] = 2\n",
    "\n",
    "            for token_index in range(len(tokens_sentnece_list)):\n",
    "\n",
    "                token = tokens_sentnece_list[token_index]\n",
    "\n",
    "                if tokens_sentnece_list_masking[token_index] == 2:\n",
    "                    # NER MASK\n",
    "                    continue\n",
    "\n",
    "                if random.random() <= prob_token:\n",
    "                    tokens_sentnece_list_masking[token_index] = 1\n",
    "                else :\n",
    "                    tokens_sentnece_list_masking[token_index] = 0\n",
    "\n",
    "            result = []\n",
    "\n",
    "            for i in range(len(tokens_sentnece_list_masking)):\n",
    "                if tokens_sentnece_list_masking[i] == 2:\n",
    "                    # if prev is mask2 then do not repeat\n",
    "                    if i > 0 and tokens_sentnece_list_masking[i - 1] == 2:\n",
    "                        continue\n",
    "\n",
    "                    result.append(NER_MASK)\n",
    "\n",
    "                elif tokens_sentnece_list_masking[i] == 1:\n",
    "                    # if prev is mask1 then do not repeat\n",
    "                    if i > 0 and tokens_sentnece_list_masking[i - 1] == 1:\n",
    "                        result.append(NER_TOKEN_MASK)\n",
    "\n",
    "                else:\n",
    "                    result.append(tokens_sentnece_list[i])\n",
    "\n",
    "            processed = \" \".join(result)\n",
    "\n",
    "            label = \" \".join(np.where(tokens_sentnece_list_masking, tokens_sentnece_list, \"<unmasked>\"))\n",
    "\n",
    "            return [processed, label, tokens_sentnece_list_masking]\n",
    "\n",
    "        def sentence_scorer(sentence, text, entities, rouge_score):\n",
    "\n",
    "            # count entities\n",
    "            entites_count = 0\n",
    "            for entity in entities:\n",
    "                if entity in sentence:\n",
    "                    entites_count += 1\n",
    "            entites_score = (entites_count * 1.0) / 4\n",
    "\n",
    "            # bertscore\n",
    "            #bertscore_results = bertscore.compute(predictions=[sentence], references=[text], lang='en')\n",
    "\n",
    "    #        bertscore_value = np.average(bertscore_results[\"f1\"])\n",
    "\n",
    "            return entites_score + rouge_score# + bertscore_value\n",
    "\n",
    "\n",
    "        def to_sentences(text):\n",
    "            sentences = nltk.sent_tokenize(text)\n",
    "            texts = [get_other_text(sent, text) for sent in sentences]\n",
    "\n",
    "            return (texts, sentences)\n",
    "\n",
    "        def custom_train_preprocess(examples):\n",
    "\n",
    "            #batch_texts = \". \".join(examples[text_column])\n",
    "\n",
    "\n",
    "            list_entities = []\n",
    "\n",
    "            #list_entities = extract_entties_with_spacy(batch_texts, spacy_pipeline)\n",
    "\n",
    "            all_texts = []\n",
    "            all_sentences = []\n",
    "            groups = []\n",
    "            for i in range(len(examples[text_column])):\n",
    "\n",
    "                preprocessed_exp = examples[text_column][i]\n",
    "                \n",
    "                \n",
    "                texts, sentences = to_sentences(preprocessed_exp)\n",
    "                start_index = len(all_texts)\n",
    "            #   for tex in texts:\n",
    "            #        all_texts.append(tex)\n",
    "            #    for sen in sentences:\n",
    "            #        all_sentences.append(sen)\n",
    "                all_texts.extend(texts)\n",
    "                all_sentences.extend(sentences)\n",
    "                end_index = len(all_texts)\n",
    "                groups.append((start_index, end_index, sentences))\n",
    "\n",
    "            # compute rouge score for the whole batch\n",
    "\n",
    "            result_rouge = rouge.compute(predictions=all_sentences, references=all_texts, rouge_types=[\"rouge1\"], use_aggregator=False)\n",
    "            rouge_scores = [score for score in result_rouge.get(\"rouge1\")]\n",
    "            #rouge_scores = [random.random() for sent in all_sentences]\n",
    "\n",
    "\n",
    "            documents = []\n",
    "            summaries = []\n",
    "            mlm_labels = []\n",
    "            for i in range(len(examples[text_column])):\n",
    "\n",
    "                preprocessed_exp = examples[text_column][i]\n",
    "                \n",
    "                if len(preprocessed_exp) == 0:\n",
    "                    mlm_labels.append(\"\")\n",
    "                    summaries.append(\"\")\n",
    "                    documents.append(preprocessed_exp)\n",
    "                    \n",
    "                    continue\n",
    "    #            list_entities = extract_entties_with_spacy(preprocessed_exp, spacy_pipeline)\n",
    "                list_entities = examples[\"ner\"][i]\n",
    "\n",
    "              #  texts, sentences = to_sentences(preprocessed_exp)\n",
    "\n",
    "                try:\n",
    "                    \n",
    "                    sentences = groups[i][2]\n",
    "                    this_rouge_scores = rouge_scores[groups[i][0]:groups[i][1]]\n",
    "                except:\n",
    "                    print(i)\n",
    "                    print(groups[i])\n",
    "                    raise(\"error\")\n",
    "             #   result_rouge = rouge.compute(predictions=sentences, references=texts, use_stemmer=True, use_aggregator=False)\n",
    "              #  rouge_scores = [score.fmeasure for score in result_rouge.get(\"rouge1\")]\n",
    "\n",
    "                examples_data = {\n",
    "                    \"sentence\": sentences,\n",
    "               #     \"rouge1\": rouge_scores\n",
    "                    \"rouge1\": this_rouge_scores\n",
    "                }\n",
    "\n",
    "                df = pd.DataFrame(examples_data)\n",
    "                df[\"score\"] = df.apply(lambda x: sentence_scorer(x[\"sentence\"], preprocessed_exp, list_entities, x[\"rouge1\"]), axis=1)\n",
    "                df.sort_values(\"score\", ascending=False, inplace=True)\n",
    "                df[\"normalized_score\"] = df[\"score\"].map(lambda x: x / (df[\"score\"].max() + 0.001))\n",
    "                masked_labels = df.apply(\n",
    "                    lambda x: \n",
    "                        mask_sentence(x[\"sentence\"], list_entities, data_args[\"ner_mlm_prob\"], x[\"normalized_score\"] * (0.1 + data_args[\"ner_mlm_prob\"]))\n",
    "                              , \n",
    "                    axis=1)\n",
    "                \n",
    "                try:\n",
    "#                    print(masked_labels)\n",
    "                    df[\"masked_labels\"] = masked_labels\n",
    "                except:\n",
    "                    print(i)\n",
    "                    print(\"preprocessed\", preprocessed_exp, \"end\")\n",
    "                    raise 'error'\n",
    "                    \n",
    "                df[\"masked\"] = df[\"masked_labels\"].map(lambda x: x[0] )\n",
    "                df[\"labels\"] = df[\"masked_labels\"].map(lambda x: x[1])\n",
    "\n",
    "                #df[\"num_masked1\"] = df[\"masked\"].map(lambda x: x.count(\"<mask1>\"))\n",
    "                #df[\"num_masked2\"] = df[\"masked\"].map(lambda x: x.count(\"<mask2>\"))\n",
    "                #df[\"length\"] = df[\"sentence\"].map(lambda x: len(x.split()))\n",
    "                #df[\"length_masked\"] = df[\"masked\"].map(lambda x: len(x.split()))\n",
    "                #df[\"masked_prob\"] = df.apply(lambda x: ((x[\"num_masked2\"] + x[\"num_masked1\"]) + (x[\"length\"] - x[\"length_masked\"])) / x[\"length\"], axis=1)\n",
    "\n",
    "\n",
    "                #print(df)\n",
    "\n",
    "                if data_args[\"ner_mlm\"]:\n",
    "                    # mlm for faithfull\n",
    "                    summary = \" \".join(df[\"sentence\"].sort_index( ))\n",
    "                elif data_args[\"ner_sgs_mlm\"]:\n",
    "                    # mlm-sgs for faithfull\n",
    "                    num_sentences = df.shape[0] * 0.25\n",
    "                    num_sentences = int(round(num_sentences, 0))\n",
    "                    num_sentences = 1 if num_sentences == 0 else num_sentences\n",
    "                    summary = \" \".join(df.head(num_sentences).sort_index( )[\"sentence\"])\n",
    "\n",
    "                preprocessed_exp = \" \".join(df[\"masked\"].sort_index())\n",
    "                if data_args[\"ner_mlm\"]:\n",
    "                    preprocessed_exp = \" \".join([MLM_CONNECTOR, preprocessed_exp])\n",
    "                elif  data_args[\"ner_sgs_mlm\"]:\n",
    "                    preprocessed_exp = \" \".join([MLM_CONNECTOR, MLM_SGS_CONNECTOR, preprocessed_exp])\n",
    "\n",
    "\n",
    "                #print(df.sort_index())\n",
    "                #print(df.describe())\n",
    "\n",
    "                mlm_labels.append(\" \".join(df[\"labels\"].sort_index( )))\n",
    "                summaries.append(summary)\n",
    "                documents.append(preprocessed_exp)\n",
    "\n",
    "            new_examples = {}\n",
    "            new_examples[text_column] = documents\n",
    "            new_examples[summary_column] = summaries\n",
    "            new_examples[\"mlm_label\"] = mlm_labels\n",
    "            return new_examples\n",
    "\n",
    "        return dataset.select(ranges).map(\n",
    "            custom_train_preprocess,\n",
    "            batched=True,\n",
    "            num_proc=num_proc,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "    \n",
    "    \n",
    "    ner_dataset = gpu_spacy_process(dataset, ranges, batch_size)\n",
    "    \n",
    "    processed_dataset = pre_process_dataset(ner_dataset, num_proc, range(len(ner_dataset)), batch_size)\n",
    "    \n",
    "    processed_dataset.save_to_disk(output_dir)   \n",
    "    \n",
    "    return processed_dataset\n",
    "                \n",
    "#gpu_spacy_process(2, 20000, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1f8c7f1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5665cd56611647caaa6018debb525a20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/5.76k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d8f580f87b44a0bbba0af24f85cb865",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/6.24k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#from datasets import load_from_disk\n",
    "\n",
    "xsum_dataset = load_dataset(\n",
    "    \"xsum\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19dec45e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "484253249c3a437aa9d619989293466d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['document', 'summary', 'id', 'ner'],\n",
       "    num_rows: 20000\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu_spacy_process(xsum_dataset[\"train\"], 2, range(20000), 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b53560f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf38b05751a94a15bf015339d49fc812",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/204045 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85be1fa4fae541b5bd2c5e0c826f0888",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=10):   0%|          | 0/204045 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c93d0a6bd8940e0ab71de32412d5247",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/4 shards):   0%|          | 0/204045 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_dir = r\"G:\\.cache\\huggingface\\datasets\\mlm\\xsum_slg_mlm\"\n",
    "sample = process_ds(\n",
    "    xsum_dataset[\"train\"], \n",
    "    output_dir, \n",
    "    10, \n",
    "    range(len(xsum_dataset[\"train\"])), \n",
    "    1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76e48d56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f939d385d9154a12822a57556b485603",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=10):   0%|          | 0/204045 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3004d450b15c4065af14677cd3929810",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/3 shards):   0%|          | 0/204045 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_dir = r\"G:\\.cache\\huggingface\\datasets\\mlm\\xsum_slg_mlm\"\n",
    "sample = process_ds(\n",
    "    xsum_dataset[\"train\"], \n",
    "    output_dir, \n",
    "    10, \n",
    "    range(len(xsum_dataset[\"train\"])), \n",
    "    1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e885a411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document': '<conn1> <conn2> full of <mask2> <mask1> the areas worst affected is still being <mask1> <mask2> ongoing in <mask1> <mask2> <mask2> <mask2> <mask1> remain badly <mask2> <mask2> <mask2> Trains on <mask1> mainline face due to at the Lamington Viaduct <mask2> and householders were affected by <mask2> <mask2> <mask2> <mask2> <mask1> overflowed into town <mask1> Minister <mask1> visited the area to inspect damage The waters breached a retaining wall many commercial properties on <mask1> the main shopping thoroughfare Jeanette Tate owns <mask1> which badly affected said <mask2> not fault the multiagency response once the flood However she said more preventative work could have been carried out to ensure the retaining wall did not fail It is difficult I do think is so much publicity <mask1> the Nith and I totally appreciate that but it is almost like were neglected or forgotten she said That may not be true it is perhaps my perspective over last few days Why were you not ready to help us a bit when warning the alarm alerts had gone Meanwhile a alert remains in place across <mask1> of constant rain <mask1> badly hit by problems sparking calls to introduce more defences in the area <mask1> Council put <mask2> <mask2> its website of roads affected and drivers been urged <mask2> <mask2> closure The Labour <mask2> <mask2> leader Alex was <mask1> <mask1> to <mask2> <mask2> <mask1> He said it was important to get the flood protection plan right but backed calls to speed up the process I was quite taken aback by the amount damage that been done he said Obviously it is heartbreaking for people who have been forced out of their homes and the impact on businesses He said it was important that immediate steps were taken to protect the areas most vulnerable and a clear timetable put in place for flood prevention plans Have you been affected flooding <mask1> and <mask1> or the <mask1> Tell us about your experience of the situation and how it was handled Email us on selkirknewsbbccouk or dumfriesbbccouk',\n",
       " 'summary': \"The full cost of damage in Newton Stewart, one of the areas worst affected, is still being assessed. Repair work is ongoing in Hawick and many roads in Peeblesshire remain badly affected by standing water. Many businesses and householders were affected by flooding in Newton Stewart after the River Cree overflowed into the town. Scottish Borders Council has put a list on its website of the roads worst affected and drivers have been urged not to ignore closure signs. The Labour Party's deputy Scottish leader Alex Rowley was in Hawick on Monday to see the situation first hand. Have you been affected by flooding in Dumfries and Galloway or the Borders?\",\n",
       " 'id': '35232142',\n",
       " 'ner': ['Newton Stewart',\n",
       "  'one',\n",
       "  'Repair',\n",
       "  'Hawick',\n",
       "  'Peeblesshire',\n",
       "  'Trains',\n",
       "  'west coast',\n",
       "  'the Lamington Viaduct',\n",
       "  'Newton Stewart',\n",
       "  'the River Cree',\n",
       "  'First',\n",
       "  'Nicola Sturgeon',\n",
       "  'Victoria Street',\n",
       "  'Jeanette Tate',\n",
       "  'the Cinnamon Cafe',\n",
       "  'Dumfries and the Nith',\n",
       "  'the last few days',\n",
       "  'Borders',\n",
       "  'Peebles',\n",
       "  'Scottish Borders Council',\n",
       "  \"The Labour Party's\",\n",
       "  'Scottish',\n",
       "  'Alex Rowley',\n",
       "  'Hawick',\n",
       "  'Monday',\n",
       "  'first',\n",
       "  'Dumfries',\n",
       "  'Galloway',\n",
       "  'Borders',\n",
       "  'selkirk.news@bbc.co.uk'],\n",
       " 'mlm_label': 'The <unmasked> cost <unmasked> damage in Newton Stewart one of <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> assessed Repair work is <unmasked> <unmasked> Hawick and many roads in Peeblesshire <unmasked> <unmasked> affected by standing water <unmasked> <unmasked> the west coast <unmasked> <unmasked> disruption <unmasked> <unmasked> damage <unmasked> <unmasked> <unmasked> <unmasked> Many businesses <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> flooding in Newton Stewart after the River Cree <unmasked> <unmasked> the <unmasked> First <unmasked> Nicola Sturgeon <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> the <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> flooding <unmasked> <unmasked> <unmasked> <unmasked> Victoria Street <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> who <unmasked> the Cinnamon Cafe <unmasked> was <unmasked> <unmasked> <unmasked> she could <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> hit <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> but <unmasked> <unmasked> <unmasked> there <unmasked> <unmasked> <unmasked> <unmasked> for Dumfries and <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> but <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> the <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> more <unmasked> the <unmasked> and <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> out <unmasked> <unmasked> flood <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> the Borders because <unmasked> the <unmasked> <unmasked> Peebles was <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> Scottish Borders <unmasked> has <unmasked> a list on <unmasked> <unmasked> <unmasked> the <unmasked> worst <unmasked> <unmasked> <unmasked> have <unmasked> <unmasked> not to ignore <unmasked> signs <unmasked> <unmasked> Partys deputy Scottish <unmasked> <unmasked> Rowley <unmasked> in Hawick on Monday <unmasked> see the situation first hand <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> of <unmasked> <unmasked> has <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> by <unmasked> in Dumfries <unmasked> Galloway <unmasked> <unmasked> Borders <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked> <unmasked>'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd3b0a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\n",
    "    \"ccdv/arxiv-summarization\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5a65974",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['article', 'abstract'],\n",
       "        num_rows: 203037\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['article', 'abstract'],\n",
       "        num_rows: 6436\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['article', 'abstract'],\n",
       "        num_rows: 6440\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad02625e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a61fc61e150549c7bd20b56fc54c5c2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/203037 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_dir = r\"G:\\.cache\\huggingface\\datasets\\mlm\\arxiv\"\n",
    "sample = process_ds(\n",
    "    dataset[\"train\"], \n",
    "    output_dir, \n",
    "    10, \n",
    "    range(len(dataset[\"train\"])), \n",
    "    100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f188db0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ds(dataset, output_dir, num_proc = 1, ranges = range(100), batch_size = 1000):\n",
    "    \n",
    "    def max_length(dataset, num_proc, ranges, batch_size):\n",
    "        x = []\n",
    "        def process(examples):\n",
    "            articles = []\n",
    "            for e in examples[\"article\"]:\n",
    "                articles.append(\n",
    "                    \" \".join(e.split()[0:1024])\n",
    "                )\n",
    "\n",
    "            examples[\"article\"] = articles\n",
    "        \n",
    "            return examples\n",
    "        \n",
    "        return dataset.select(ranges).map(\n",
    "            process,\n",
    "            batched=True,\n",
    "            num_proc=num_proc,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "    \n",
    "    def gpu_spacy_process(dataset, ranges = range(100), batch_size = 1000):\n",
    "\n",
    "        from thinc.api import set_gpu_allocator, require_gpu\n",
    "\n",
    "        import spacy\n",
    "        def process(examples):\n",
    "\n",
    "            require_gpu(0)\n",
    "            # Use the GPU, with memory allocations directed via PyTorch.\n",
    "            # This prevents out-of-memory errors that would otherwise occur from competing\n",
    "            # memory pools.\n",
    "            #set_gpu_allocator(\"pytorch\")\n",
    "            #require_gpu(1 if random.random() < 0.3 else 0)\n",
    "            try:\n",
    "                spacy_pipeline = spacy.load('en_core_web_sm', enable=[\"ner\"])\n",
    "            except OSError:\n",
    "                logging.warning(\"Downloading language model for the spaCy model.\")\n",
    "                from spacy.cli import download\n",
    "                download('en_core_web_sm')\n",
    "                spacy_pipeline = spacy.load('en_core_web_sm')\n",
    "\n",
    "            texts = examples[\"article\"]\n",
    "            list_entities = []\n",
    "\n",
    "            spacy_docs = spacy_pipeline.pipe(texts)\n",
    "            for doc in spacy_docs:\n",
    "                list_entities.append([a.text for a in doc.ents])\n",
    "\n",
    "            examples[\"ner\"] = list_entities\n",
    "            return examples\n",
    "\n",
    "        return dataset.select(ranges).map(\n",
    "            process,\n",
    "            batched=True,\n",
    "            num_proc=2,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "    \n",
    "    def pre_process_dataset(dataset, num_proc = 1, ranges = range(100), batch_size = 1000):\n",
    "        import string\n",
    "        import random\n",
    "\n",
    "\n",
    "        import pandas as pd\n",
    "        import random    \n",
    "        import nltk\n",
    "        import math\n",
    "\n",
    "        import evaluate\n",
    "        import numpy as np\n",
    "\n",
    "        from datasets import load_dataset, load_metric\n",
    "        data_args = {\n",
    "            \"ner_mlm\": True,\n",
    "            \"ner_sgs_mlm\": False,\n",
    "            \"ner_mlm_prob\": 0.35\n",
    "        }\n",
    "\n",
    "        NER_MASK = \"<mask1>\"\n",
    "        NER_TOKEN_MASK = \"<mask2>\"\n",
    "        MLM_CONNECTOR = \"<conn1>\"\n",
    "        MLM_SGS_CONNECTOR = \"<conn2>\"\n",
    "\n",
    "        text_column = \"article\"\n",
    "        summary_column = \"abstract\"\n",
    "\n",
    "        #bertscore = load_metric('bertscore')\n",
    "\n",
    "        rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "\n",
    "        def get_other_text(sentence, text):\n",
    "            res = text.replace(sentence, \"\", 1)\n",
    "            if (len(res) == 0 and len(res.split()) == 0):\n",
    "                res = text\n",
    "            return res\n",
    "\n",
    "        def extract_entties_with_spacy(source, spacy_pipeline):\n",
    "\n",
    "            list_entities = []\n",
    "\n",
    "            spacy_doc = spacy_pipeline(source)#, n_process=-1)\n",
    "            list_entities = [a.text for a in spacy_doc.ents]\n",
    "\n",
    "            return list_entities\n",
    "\n",
    "\n",
    "        def mask_sentence(sentence, entities, prob_ner, prob_token):\n",
    "\n",
    "            tokens_sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "            processed = sentence\n",
    "            tokens_sentnece_list = tokens_sentence.split()\n",
    "\n",
    "            tokens_sentnece_list_masking = np.zeros(len(tokens_sentnece_list))\n",
    "\n",
    "            for entity in entities:\n",
    "                if entity in sentence:\n",
    "                    tokens_sentence = tokens_sentence.replace(entity, \"\")\n",
    "                    if random.random() <= prob_ner:\n",
    "                        entity_tokens = entity.split()\n",
    "                        entity_length = len(entity_tokens)\n",
    "                        for tokens_index in range(len(tokens_sentnece_list)):\n",
    "                            tokens_seq = \" \".join(tokens_sentnece_list[tokens_index:tokens_index + entity_length])\n",
    "                            entity_seq = entity\n",
    "                            if tokens_seq == entity_seq:\n",
    "                                tokens_sentnece_list_masking[tokens_index:tokens_index + entity_length] = 2\n",
    "\n",
    "            for token_index in range(len(tokens_sentnece_list)):\n",
    "\n",
    "                token = tokens_sentnece_list[token_index]\n",
    "\n",
    "                if tokens_sentnece_list_masking[token_index] == 2:\n",
    "                    # NER MASK\n",
    "                    continue\n",
    "\n",
    "                if random.random() <= prob_token:\n",
    "                    tokens_sentnece_list_masking[token_index] = 1\n",
    "                else :\n",
    "                    tokens_sentnece_list_masking[token_index] = 0\n",
    "\n",
    "            result = []\n",
    "\n",
    "            for i in range(len(tokens_sentnece_list_masking)):\n",
    "                if tokens_sentnece_list_masking[i] == 2:\n",
    "                    # if prev is mask2 then do not repeat\n",
    "                    if i > 0 and tokens_sentnece_list_masking[i - 1] == 2:\n",
    "                        continue\n",
    "\n",
    "                    result.append(NER_MASK)\n",
    "\n",
    "                elif tokens_sentnece_list_masking[i] == 1:\n",
    "                    # if prev is mask1 then do not repeat\n",
    "                    if i > 0 and tokens_sentnece_list_masking[i - 1] == 1:\n",
    "                        result.append(NER_TOKEN_MASK)\n",
    "\n",
    "                else:\n",
    "                    result.append(tokens_sentnece_list[i])\n",
    "\n",
    "            processed = \" \".join(result)\n",
    "\n",
    "            label = \" \".join(np.where(tokens_sentnece_list_masking, tokens_sentnece_list, \"<unmasked>\"))\n",
    "\n",
    "            return [processed, label, tokens_sentnece_list_masking]\n",
    "\n",
    "        def sentence_scorer(sentence, text, entities, rouge_score):\n",
    "\n",
    "            # count entities\n",
    "            entites_count = 0\n",
    "            for entity in entities:\n",
    "                if entity in sentence:\n",
    "                    entites_count += 1\n",
    "            entites_score = (entites_count * 1.0) / 4\n",
    "\n",
    "            # bertscore\n",
    "            #bertscore_results = bertscore.compute(predictions=[sentence], references=[text], lang='en')\n",
    "\n",
    "    #        bertscore_value = np.average(bertscore_results[\"f1\"])\n",
    "\n",
    "            return entites_score + rouge_score# + bertscore_value\n",
    "\n",
    "\n",
    "        def to_sentences(text):\n",
    "            sentences = nltk.sent_tokenize(text)\n",
    "            texts = [get_other_text(sent, text) for sent in sentences]\n",
    "\n",
    "            return (texts, sentences)\n",
    "\n",
    "        def custom_train_preprocess(examples):\n",
    "\n",
    "            #batch_texts = \". \".join(examples[text_column])\n",
    "\n",
    "\n",
    "            list_entities = []\n",
    "\n",
    "            #list_entities = extract_entties_with_spacy(batch_texts, spacy_pipeline)\n",
    "\n",
    "            all_texts = []\n",
    "            all_sentences = []\n",
    "            groups = []\n",
    "            for i in range(len(examples[text_column])):\n",
    "\n",
    "                preprocessed_exp = examples[text_column][i]\n",
    "                \n",
    "                \n",
    "                texts, sentences = to_sentences(preprocessed_exp)\n",
    "                start_index = len(all_texts)\n",
    "            #   for tex in texts:\n",
    "            #        all_texts.append(tex)\n",
    "            #    for sen in sentences:\n",
    "            #        all_sentences.append(sen)\n",
    "                all_texts.extend(texts)\n",
    "                all_sentences.extend(sentences)\n",
    "                end_index = len(all_texts)\n",
    "                groups.append((start_index, end_index, sentences))\n",
    "\n",
    "            # compute rouge score for the whole batch\n",
    "\n",
    "            result_rouge = rouge.compute(predictions=all_sentences, references=all_texts, rouge_types=[\"rouge1\"], use_aggregator=False)\n",
    "            rouge_scores = [score for score in result_rouge.get(\"rouge1\")]\n",
    "            #rouge_scores = [random.random() for sent in all_sentences]\n",
    "\n",
    "\n",
    "            documents = []\n",
    "            summaries = []\n",
    "            mlm_labels = []\n",
    "            masked_docuemnts = []\n",
    "            for i in range(len(examples[text_column])):\n",
    "                \n",
    "                document = \"\"\n",
    "                summary = \"\"\n",
    "                masked_document = \"\"\n",
    "                mlm_label = \"\"\n",
    "                \n",
    "                preprocessed_exp = examples[text_column][i]\n",
    "                \n",
    "                if len(preprocessed_exp) == 0:\n",
    "                    mlm_labels.append(\"\")\n",
    "                    summaries.append(examples[summary_column][i])\n",
    "                    documents.append(preprocessed_exp)\n",
    "                    masked_docuemnts.append(\"\")\n",
    "                    continue\n",
    "    #            list_entities = extract_entties_with_spacy(preprocessed_exp, spacy_pipeline)\n",
    "                list_entities = examples[\"ner\"][i]\n",
    "\n",
    "              #  texts, sentences = to_sentences(preprocessed_exp)\n",
    "\n",
    "                try:\n",
    "                    \n",
    "                    sentences = groups[i][2]\n",
    "                    this_rouge_scores = rouge_scores[groups[i][0]:groups[i][1]]\n",
    "                except:\n",
    "                    print(i)\n",
    "                    print(groups[i])\n",
    "                    raise(\"error\")\n",
    "             #   result_rouge = rouge.compute(predictions=sentences, references=texts, use_stemmer=True, use_aggregator=False)\n",
    "              #  rouge_scores = [score.fmeasure for score in result_rouge.get(\"rouge1\")]\n",
    "\n",
    "                examples_data = {\n",
    "                    \"sentence\": sentences,\n",
    "               #     \"rouge1\": rouge_scores\n",
    "                    \"rouge1\": this_rouge_scores\n",
    "                }\n",
    "\n",
    "                df = pd.DataFrame(examples_data)\n",
    "                df[\"score\"] = df.apply(lambda x: sentence_scorer(x[\"sentence\"], preprocessed_exp, list_entities, x[\"rouge1\"]), axis=1)\n",
    "                df.sort_values(\"score\", ascending=False, inplace=True)\n",
    "                df[\"normalized_score\"] = df[\"score\"].map(lambda x: x / (df[\"score\"].max() + 0.001))\n",
    "                masked_labels = df.apply(\n",
    "                    lambda x: \n",
    "                        mask_sentence(x[\"sentence\"], list_entities, data_args[\"ner_mlm_prob\"], x[\"normalized_score\"] * (0.1 + data_args[\"ner_mlm_prob\"]))\n",
    "                              , \n",
    "                    axis=1)\n",
    "                \n",
    "                try:\n",
    "#                    print(masked_labels)\n",
    "                    df[\"masked_labels\"] = masked_labels\n",
    "                except:\n",
    "                    print(i)\n",
    "                    print(\"preprocessed\", preprocessed_exp, \"end\")\n",
    "                    raise 'error'\n",
    "                    \n",
    "                df[\"masked\"] = df[\"masked_labels\"].map(lambda x: x[0] )\n",
    "                df[\"labels\"] = df[\"masked_labels\"].map(lambda x: x[1])\n",
    "\n",
    "                #df[\"num_masked1\"] = df[\"masked\"].map(lambda x: x.count(\"<mask1>\"))\n",
    "                #df[\"num_masked2\"] = df[\"masked\"].map(lambda x: x.count(\"<mask2>\"))\n",
    "                #df[\"length\"] = df[\"sentence\"].map(lambda x: len(x.split()))\n",
    "                #df[\"length_masked\"] = df[\"masked\"].map(lambda x: len(x.split()))\n",
    "                #df[\"masked_prob\"] = df.apply(lambda x: ((x[\"num_masked2\"] + x[\"num_masked1\"]) + (x[\"length\"] - x[\"length_masked\"])) / x[\"length\"], axis=1)\n",
    "\n",
    "\n",
    "                #print(df)\n",
    "\n",
    "                if data_args[\"ner_mlm\"]:\n",
    "                    # mlm for faithfull\n",
    "                    num_sentences = 0\n",
    "                    check_words = \"\"\n",
    "                    for index in df.index:\n",
    "                        check_words = \" \".join([check_words, df.loc[index][\"sentence\"]])\n",
    "                        num_sentences += 1\n",
    "                        if (len(check_words.split()) > 512):\n",
    "                            break\n",
    "\n",
    "                    selectedDF = df.head(num_sentences).sort_index()\n",
    "                    document =  \" \".join(selectedDF[\"sentence\"])\n",
    "                    summary =  examples[summary_column][i]\n",
    "                    masked_document =  \" \".join(selectedDF[\"masked\"])\n",
    "                    mlm_label =  \" \".join(selectedDF[\"labels\"])\n",
    "                    #print(document)\n",
    "                    #print(summary)\n",
    "                    #document = \n",
    "                    #summary = \" \".join(df[\"sentence\"].sort_index( ))\n",
    "                    \n",
    "           #     elif data_args[\"ner_sgs_mlm\"]:\n",
    "           #         # mlm-sgs for faithfull\n",
    "           #         num_sentences = df.shape[0] * 0.25\n",
    "           #         num_sentences = int(round(num_sentences, 0))\n",
    "           #         num_sentences = 1 if num_sentences == 0 else num_sentences\n",
    "           #         summary = \" \".join(df.head(num_sentences).sort_index( )[\"sentence\"])\n",
    "\n",
    "                #preprocessed_exp = \" \".join(df[\"masked\"].sort_index())\n",
    "                if data_args[\"ner_mlm\"]:\n",
    "                    masked_document = \" \".join([MLM_CONNECTOR, masked_document])\n",
    "                #elif  data_args[\"ner_sgs_mlm\"]:\n",
    "                 #   preprocessed_exp = \" \".join([MLM_CONNECTOR, MLM_SGS_CONNECTOR, preprocessed_exp])\n",
    "\n",
    "\n",
    "                #print(df.sort_index())\n",
    "                #print(df.describe())\n",
    "\n",
    "                #mlm_labels.append(\" \".join(df[\"labels\"].sort_index( )))\n",
    "                mlm_labels.append(mlm_label)\n",
    "                masked_docuemnts.append(masked_document)\n",
    "                summaries.append(summary)\n",
    "                documents.append(document)\n",
    "\n",
    "            new_examples = {}\n",
    "            new_examples[text_column] = documents\n",
    "            new_examples[summary_column] = summaries\n",
    "            new_examples[\"mlm_label\"] = mlm_labels\n",
    "            new_examples[\"masked_document\"] = masked_docuemnts\n",
    "            return new_examples\n",
    "\n",
    "        return dataset.select(ranges).map(\n",
    "            custom_train_preprocess,\n",
    "            batched=True,\n",
    "            num_proc=num_proc,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "    \n",
    "    maxed_length_dataset = max_length(dataset, num_proc, ranges, batch_size)\n",
    "    \n",
    "    ner_dataset = gpu_spacy_process(maxed_length_dataset, range(len(maxed_length_dataset)), batch_size)\n",
    "    \n",
    "    processed_dataset = pre_process_dataset(ner_dataset, num_proc, range(len(ner_dataset)), batch_size)\n",
    "    \n",
    "    processed_dataset.save_to_disk(output_dir)   \n",
    "    \n",
    "    return processed_dataset\n",
    "                \n",
    "#gpu_spacy_process(2, 20000, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8114444d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04ef0880e7c24ef8a0f4d59f99eeeed3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=10):   0%|          | 0/203037 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eb5a7fe64054b0b8005a13f69522a60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/5 shards):   0%|          | 0/203037 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_dir = r\"G:\\.cache\\huggingface\\datasets\\mlm\\arxiv_0.3_1024\"\n",
    "sample = process_ds(\n",
    "    dataset[\"train\"], \n",
    "    output_dir, \n",
    "    10, \n",
    "    range(len(dataset[\"train\"])), \n",
    "    1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04cc9bcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baab9c2ccfc042e0a09e013be85a86ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=10):   0%|          | 0/6436 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "688bb342b2af493dbd447e3734d7444d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/6436 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_dir = r\"G:\\.cache\\huggingface\\datasets\\mlm\\arxiv_val_0.3_1024\"\n",
    "sample = process_ds(\n",
    "    dataset[\"validation\"], \n",
    "    output_dir, \n",
    "    10, \n",
    "    range(len(dataset[\"validation\"])), \n",
    "    1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92de62ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "032a412ab9564237a512a93811bd6040",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=10):   0%|          | 0/6440 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48d37326360c4a44a1539ea0efc857e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/6440 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_dir = r\"G:\\.cache\\huggingface\\datasets\\mlm\\arxiv_test_0.3_1024\"\n",
    "sample = process_ds(\n",
    "    dataset[\"test\"], \n",
    "    output_dir, \n",
    "    10, \n",
    "    range(len(dataset[\"test\"])), \n",
    "    1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "b6406752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['article', 'abstract', 'ner', 'mlm_label', 'masked_document'],\n",
       "    num_rows: 6440\n",
       "})"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "a88888a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'we have studied the leptonic decay @xmath0 , via the decay channel @xmath1 , using a sample of tagged @xmath2 decays collected near the @xmath3 peak production energy in @xmath4 collisions with the cleo - c detector . \\n we obtain @xmath5 and determine the decay constant @xmath6  mev , where the first uncertainties are statistical and the second are systematic .'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[1][\"abstract\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "039d1af7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the leptonic decays of a charged pseudoscalar meson @xmath7 are processes of the type @xmath8 , where @xmath9 , @xmath10 , or @xmath11 . because no strong interactions are present in the leptonic final state @xmath12 , such decays provide a clean way to probe the complex , strong interactions that bind the quark and antiquark within the initial - state meson . in these decays , strong interaction effects can be parametrized by a single quantity , @xmath13 , the pseudoscalar meson decay constant . the leptonic decay rate can be measured by experiment , and the decay constant can be determined by the equation ( ignoring radiative corrections ) @xmath14 where @xmath15 is the fermi coupling constant , @xmath16 is the cabibbo - kobayashi - maskawa ( ckm ) matrix @xcite element , @xmath17 is the mass of the meson , and @xmath18 is the mass of the charged lepton . the quantity @xmath13 describes the amplitude for the @xmath19 and @xmath20-quarks within the @xmath21 to have zero separation , a condition necessary for them to annihilate into the virtual @xmath22 boson that produces the @xmath12 pair . the experimental determination of decay constants is one of the most important tests of calculations involving nonperturbative qcd . such calculations have been performed using various models @xcite or using lattice qcd ( lqcd ) . the latter is now generally considered to be the most reliable way to calculate the quantity . knowledge of decay constants is important for describing several key processes , such as @xmath23 mixing , which depends on @xmath24 , a quantity that is also predicted by lqcd calculations . experimental determination @xcite of @xmath24 with the leptonic decay of a @xmath25 meson is , however , very limited as the rate is highly suppressed due to the smallness of the magnitude of the relevant ckm matrix element @xmath26 . the charm mesons , @xmath27 and @xmath28 , are better instruments to study the leptonic decays of heavy mesons since these decays are either less ckm suppressed or favored , _ i.e. _ , @xmath29 and @xmath30 are much larger than @xmath31 . thus , the decay constants @xmath32 and @xmath33 determined from charm meson decays can be used to test and validate the necessary lqcd calculations applicable to the @xmath34-meson sector . among the leptonic decays in the charm - quark sector , @xmath35 decays are more accessible since they are ckm favored . furthermore , the large mass of the @xmath11 lepton removes the helicity suppression that is present in the decays to lighter leptons . the existence of multiple neutrinos in the final state , however , makes measurement of this decay challenging . physics beyond the standard model ( sm ) might also affect leptonic decays of charmed mesons . depending on the non - sm features , the ratio of @xmath36 could be affected @xcite , as could the ratio @xcite @xmath37 . any of the individual widths might be increased or decreased . there is an indication of a discrepancy between the experimental determinations @xcite of @xmath33 and the most recent precision lqcd calculation @xcite . this disagreement is particularly puzzling since the cleo - c determination @xcite of @xmath32 agrees well with the lqcd calculation @xcite of that quantity . some @xcite conjecture that this discrepancy may be explained by a charged higgs boson or a leptoquark . in this article , we report an improved measurement of the absolute branching fraction of the leptonic decay @xmath0 ( charge - conjugate modes are implied ) , with @xmath1 , from which we determine the decay constant @xmath33 . we use a data sample of @xmath38 events provided by the cornell electron storage ring ( cesr ) and collected by the cleo - c detector at the center - of - mass ( cm ) energy @xmath39 mev , near @xmath3 peak production @xcite . the data sample consists of an integrated luminosity of @xmath40 @xmath41 containing @xmath42 @xmath3 pairs . we have previously reported @xcite measurements of @xmath43 and @xmath0 with a subsample of these data . a companion article @xcite reports measurements of @xmath33 from @xmath43 and @xmath0 , with @xmath44 , using essentially the same data sample as the one used in this measurement . the cleo - c detector @xcite is a general - purpose solenoidal detector with four concentric components utilized in this measurement : a small - radius six - layer stereo wire drift chamber , a 47-layer main drift chamber , a ring - imaging cherenkov ( rich ) detector , and an electromagnetic calorimeter consisting of 7800 csi(tl ) crystals . the two drift chambers operate in a @xmath45 t magnetic field and provide charged particle tracking in a solid angle of @xmath46% of @xmath47 . the chambers achieve a momentum resolution of @xmath48% at @xmath49 gev/@xmath50 . the main drift chamber also provides specific - ionization ( @xmath51 ) measurements that discriminate between charged pions and kaons . the rich detector covers approximately @xmath52% of @xmath47 and provides additional separation of pions and kaons at high momentum . the photon energy resolution of the calorimeter is @xmath53% at @xmath54 gev and @xmath55% at @xmath56 mev . electron identification is based on a likelihood variable that combines the information from the rich detector , @xmath51 , and the ratio of electromagnetic shower energy to track momentum ( @xmath57 ) . we use a geant - based @xcite monte carlo ( mc ) simulation program to study efficiency of signal - event selection and background processes . physics events are generated by evtgen @xcite , tuned with much improved knowledge of charm decays @xcite , and final - state radiation ( fsr ) is modeled by the photos @xcite program . the modeling of initial - state radiation ( isr ) is based on cross sections for @xmath3 production at lower energies obtained from the cleo - c energy scan @xcite near the cm energy where we collect the sample . the presence of two @xmath58 mesons in a @xmath3 event allows us to define a'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[1][\"article\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a34407",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
