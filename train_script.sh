deepspeed --include localhost:0,1,2,3 src/run_summarization.py --fp16 \
  --deepspeed src/deepspeed_config.json \
  --dataset_name xsum \
  --model_name_or_path facebook/bart-large \
  --do_train --evaluation_strategy no \
  --label_smoothing 0.1 --learning_rate 3e-5 --gradient_accumulation_step 4 --per_device_train_batch_size 8 \
  --max_source_length 512 --max_target_length 64 \
  --warmup_steps 500 --max_grad_norm 0.1 --max_steps 15000 --save_strategy no \
  --output_dir out_xsum --overwrite_cache --remove_unused_columns true --additional_reference_file additional_summary.txt

# # Optionally without deepspeed
python src/run_summarization.py --fp16 \
  --dataset_name xsum \
  --model_name_or_path facebook/bart-large \
  --do_train --evaluation_strategy no \
  --label_smoothing 0.1 --learning_rate 3e-5 --gradient_accumulation_step 4 --per_device_train_batch_size 8 \
  --max_source_length 512 --max_target_length 64 \
  --warmup_steps 500 --max_grad_norm 0.1 --max_steps 15000 --save_strategy no \
  --output_dir out_xsum --overwrite_cache --remove_unused_columns true

  python src/run_summarization.py --fp16  --dataset_name cnn_dailymail --dataset_config_name 3.0.0  --model_name_or_path facebook/bart-large  --do_train --evaluation_strategy no  --label_smoothing 0.1 --learning_rate 3e-5 --gradient_accumulation_step 4 --per_device_train_batch_size 8  --max_source_length 512 --max_target_length 64  --warmup_steps 500 --max_grad_norm 0.1 --max_steps 15000 --save_strategy no  --output_dir out_cnndm --overwrite_cache --remove_unused_columns true
  python src/run_summarization.py --fp16  --dataset_name xsum  --model_name_or_path facebook/bart-large  --do_train --evaluation_strategy no  --label_smoothing 0.1 --learning_rate 3e-5 --gradient_accumulation_step 4 --per_device_train_batch_size 8  --max_source_length 512 --max_target_length 64  --warmup_steps 500 --max_grad_norm 0.1 --max_steps 15000 --save_strategy no  --output_dir out_xsum --overwrite_cache --remove_unused_columns true

  python src/run_summarization.py --fp16 --predict_with_generate --num_train_epochs 10 --max_train_samples 200 --max_eval_samples 20 --max_predict_samples 20  --dataset_name cnn_dailymail --dataset_config_name 3.0.0  --model_name_or_path facebook/bart-large-cnn  --do_train --evaluation_strategy epoch  --label_smoothing 0.1 --learning_rate 3e-5 --gradient_accumulation_step 4 --per_device_train_batch_size 8  --max_source_length 512 --max_target_length 64  --warmup_steps 500 --max_grad_norm 0.1 --save_strategy no  --output_dir out_cnndm_en_dec --overwrite_cache --remove_unused_columns true

  python src/run_summarization.py --fp16 --predict_with_generate --num_train_epochs 10 --max_train_samples 200 --max_eval_samples 20 --max_predict_samples 20  --dataset_name cnn_dailymail --dataset_config_name 3.0.0  --model_name_or_path facebook/bart-large  --do_train --evaluation_strategy epoch  --label_smoothing 0.1 --learning_rate 3e-5 --gradient_accumulation_step 4 --per_device_train_batch_size 8  --max_source_length 512 --max_target_length 64  --warmup_steps 500 --max_grad_norm 0.1 --save_strategy no  --output_dir out_cnndm_en_dec --overwrite_cache --remove_unused_columns true

# xsum 512
python src/run_summarization.py --per_device_eval_batch_size 16 --include_inputs_for_metrics --fp16 --max_predict_samples 200 --dataset_name xsum  --model_name_or_path facebook/bart-large-xsum  --do_predict  --max_source_length 512 --max_target_length 64 --save_strategy no  --output_dir out_xsum_pred_1 --remove_unused_columns true --predict_with_generate
python src/run_summarization.py --per_device_eval_batch_size 16  --include_inputs_for_metrics --fp16 --dataset_name xsum  --model_name_or_path facebook/bart-large-xsum  --do_predict  --max_source_length 512 --max_target_length 64 --save_strategy no  --output_dir out_xsum_pred --remove_unused_columns true --predict_with_generate

# cnn 1024
python src/run_summarization.py  --max_predict_samples 20 --per_device_eval_batch_size 16  --include_inputs_for_metrics --fp16 --dataset_name cnn_dailymail --dataset_config_name 3.0.0  --model_name_or_path facebook/bart-large-cnn  --do_predict  --max_source_length 1024 --max_target_length 100 --save_strategy no  --output_dir out_cnn_full_pred --num_beams 6 --remove_unused_columns true --predict_with_generate
python src/run_summarization.py --per_device_eval_batch_size 16  --include_inputs_for_metrics --fp16 --dataset_name cnn_dailymail --dataset_config_name 3.0.0  --model_name_or_path facebook/bart-large-cnn  --do_predict  --max_source_length 1024 --max_target_length 100 --save_strategy no  --output_dir out_cnn_full_pred --num_beams 6 --remove_unused_columns true --predict_with_generate


# cnn 512
python src/run_summarization.py  --max_predict_samples 20 --per_device_eval_batch_size 16  --include_inputs_for_metrics --fp16 --dataset_name cnn_dailymail --dataset_config_name 3.0.0  --model_name_or_path facebook/bart-large-cnn  --do_predict  --max_source_length 512 --max_target_length 64 --save_strategy no  --output_dir out_cnn_full_pred_512 --num_beams 6 --remove_unused_columns true --predict_with_generate
python src/run_summarization.py --per_device_eval_batch_size 16  --include_inputs_for_metrics --fp16 --dataset_name cnn_dailymail --dataset_config_name 3.0.0  --model_name_or_path facebook/bart-large-cnn  --do_predict  --max_source_length 512 --max_target_length 64 --save_strategy no  --output_dir out_cnn_full_pred_512 --num_beams 6 --remove_unused_columns true --predict_with_generate

# cnn 2000-512
python src/run_summarization.py --gpu_id 0 --max_predict_samples 2000 --per_device_eval_batch_size 16  --include_inputs_for_metrics --fp16 --dataset_name cnn_dailymail --dataset_config_name 3.0.0  --model_name_or_path facebook/bart-large-cnn  --do_predict  --max_source_length 512 --max_target_length 64 --save_strategy no  --output_dir out_cnn_2000_pred_512 --num_beams 6 --remove_unused_columns true --predict_with_generate
# xsum 2000-512
python src/run_summarization.py --gpu_id 1 --max_predict_samples 2000 --per_device_eval_batch_size 16  --include_inputs_for_metrics --fp16 --dataset_name xsum --dataset_config_name 3.0.0  --model_name_or_path facebook/bart-large-xsum  --do_predict  --max_source_length 512 --max_target_length 64 --save_strategy no  --output_dir out_xsum_2000_pred_512 --num_beams 6 --remove_unused_columns true --predict_with_generate
#python src/run_summarization.py --per_device_eval_batch_size 16  --include_inputs_for_metrics --fp16 --dataset_name cnn_dailymail --dataset_config_name 3.0.0  --model_name_or_path facebook/bart-large-cnn  --do_predict  --max_source_length 512 --max_target_length 64 --save_strategy no  --output_dir out_cnn_full_pred_512 --num_beams 6 --remove_unused_columns true --predict_with_generate


#test bert
python src/run_summarization_custom.py  --num_train_epochs 10 --gpu_id 0 --max_train_samples 200 --max_eval_samples 200 --per_device_train_batch_size 8 --per_device_eval_batch_size 8  --include_inputs_for_metrics --fp16 --dataset_name xsum --dataset_config_name 3.0.0  --model_name_or_path facebook/bart-large-xsum  --do_train --do_eval  --max_source_length 512 --max_target_length 64 --overwrite_output_dir --save_strategy no  --output_dir out_xsum_200_train_512 --num_beams 6 --remove_unused_columns true --predict_with_generate  --label_smoothing 0.1 --learning_rate 3e-5 --gradient_accumulation_step 4
python src/run_summarization_custom.py  --num_train_epochs 10 --max_train_samples 200 --max_eval_samples 200 --per_device_train_batch_size 8 --per_device_eval_batch_size 8  --include_inputs_for_metrics --fp16 --dataset_name xsum --dataset_config_name 3.0.0  --model_name_or_path facebook/bart-large-xsum  --do_train --do_eval  --max_source_length 512 --max_target_length 64 --overwrite_output_dir --save_strategy no  --evaluation_strategy="epoch" --logging_strategy="epoch"  --output_dir out_xsum_200_train1_512 --num_beams 6 --remove_unused_columns true --predict_with_generate  --label_smoothing 0.1 --learning_rate 3e-5 --gradient_accumulation_step 4
python src/run_summarization_custom.py  --num_train_epochs 10 --max_train_samples 5000 --max_eval_samples 1000 --per_device_train_batch_size 8 --per_device_eval_batch_size 8  --include_inputs_for_metrics --fp16 --dataset_name xsum --dataset_config_name 3.0.0  --model_name_or_path facebook/bart-large-xsum  --do_train --do_eval  --max_source_length 512 --max_target_length 64 --overwrite_output_dir --save_strategy epoch  --evaluation_strategy=epoch --logging_strategy=epoch  --output_dir out_xsum_5000_1000_train_512 --num_beams 6 --remove_unused_columns true --predict_with_generate --warmup_steps 50  --label_smoothing 0.1 --learning_rate 3e-5 --gradient_accumulation_step 4

python src/run_summarization_custom.py  --num_train_epochs 10 --max_train_samples 200 --max_eval_samples 200 --per_device_train_batch_size 8 --per_device_eval_batch_size 8  --include_inputs_for_metrics --fp16 --dataset_name xsum --dataset_config_name 3.0.0  --model_name_or_path facebook/bart-large-xsum  --do_train --do_eval  --max_source_length 512 --max_target_length 64 --overwrite_output_dir --save_strategy epoch  --evaluation_strategy=epoch --logging_strategy=epoch  --output_dir out_xsum_200_train2_512 --num_beams 6 --remove_unused_columns true --predict_with_generate --warmup_steps 50  --label_smoothing 0.1 --learning_rate 3e-5 --gradient_accumulation_step 4

python src/run_summarization_custom.py  --num_train_epochs 6 --max_train_samples 5000 --max_eval_samples 1000 --per_device_train_batch_size 8 --per_device_eval_batch_size 8  --include_inputs_for_metrics --fp16 --dataset_name xsum  --model_name_or_path facebook/bart-large-xsum  --do_train --do_eval  --max_source_length 512 --max_target_length 64 --overwrite_output_dir --save_strategy epoch  --evaluation_strategy=epoch --logging_strategy=epoch  --output_dir out_xsum_5000_1000_train2_512 --num_beams 6 --remove_unused_columns true --predict_with_generate --warmup_steps 70  --label_smoothing 0.1 --learning_rate 3e-5 --gradient_accumulation_step 4

python src/run_summarization_custom.py  --num_train_epochs 10 --max_train_samples 200 --max_eval_samples 200 --per_device_train_batch_size 8 --per_device_eval_batch_size 8  --include_inputs_for_metrics --fp16 --dataset_name xsum --dataset_config_name 3.0.0  --model_name_or_path facebook/bart-large-xsum  --do_train --do_eval  --max_source_length 512 --max_target_length 64 --overwrite_output_dir --save_strategy epoch  --evaluation_strategy=epoch --logging_strategy=epoch  --output_dir out_xsum_200_train2_512 --num_beams 6 --remove_unused_columns true --predict_with_generate --warmup_steps 50  --label_smoothing 0.1 --learning_rate 3e-5 --gradient_accumulation_step 4 --deepspeed

python src/run_summarization_custom.py --num_train_epochs 10 --max_train_samples 200 --max_eval_samples 200 --per_device_train_batch_size 8 --per_device_eval_batch_size 8  --include_inputs_for_metrics --fp16 --dataset_name xsum --dataset_config_name 3.0.0  --model_name_or_path facebook/bart-large-xsum  --do_train --do_eval  --max_source_length 512 --max_target_length 64 --overwrite_output_dir --save_strategy epoch  --evaluation_strategy=epoch --logging_strategy=epoch  --output_dir out_xsum_200_train2_512 --num_beams 6 --remove_unused_columns true --predict_with_generate --warmup_steps 50  --label_smoothing 0.1 --learning_rate 3e-5 --gradient_accumulation_step 4

python src/run_summarization_custom.py  --num_train_epochs 5 --max_eval_samples 1000 --per_device_train_batch_size 8 --per_device_eval_batch_size 8  --include_inputs_for_metrics --fp16 --dataset_name xsum  --model_name_or_path facebook/bart-large-xsum  --do_train --do_eval --max_source_length 512 --max_target_length 64 --overwrite_output_dir --save_strategy epoch  --evaluation_strategy=epoch --logging_strategy=epoch  --output_dir out_xsum_full_1000_train2_512 --num_beams 6 --remove_unused_columns true --predict_with_generate --warmup_steps 500  --label_smoothing 0.1 --learning_rate 3e-5 --gradient_accumulation_step 4

python src/run_summarization_custom.py  --num_train_epochs 5 --max_eval_samples 1000 --per_device_train_batch_size 8 --per_device_eval_batch_size 8  --include_inputs_for_metrics --fp16 --dataset_name xsum  --model_name_or_path facebook/bart-large-xsum  --do_train --do_eval --max_source_length 512 --max_target_length 64 --overwrite_output_dir --save_strategy epoch  --evaluation_strategy=epoch --logging_strategy=epoch  --output_dir out_xsum_full_1000_train2_0307_512 --num_beams 6 --remove_unused_columns true --predict_with_generate --warmup_steps 500  --label_smoothing 0.1 --learning_rate 3e-5 --gradient_accumulation_step 4 #0406
python src/run_summarization_custom.py  --num_train_epochs 5 --max_eval_samples 1000 --per_device_train_batch_size 8 --per_device_eval_batch_size 8  --include_inputs_for_metrics --fp16 --dataset_name xsum  --model_name_or_path facebook/bart-large-xsum  --do_train --do_eval --max_source_length 512 --max_target_length 64 --overwrite_output_dir --save_strategy epoch  --evaluation_strategy=epoch --logging_strategy=epoch  --output_dir out_xsum_full_1000_train2_allbert_512 --num_beams 6 --remove_unused_columns true --predict_with_generate --warmup_steps 500  --label_smoothing 0.1 --learning_rate 3e-5 --gradient_accumulation_step 4


python src/run_summarization_custom.py --eval_steps 1000 --num_train_epochs 5 --max_train_samples 5000 --max_eval_samples 1000 --per_device_train_batch_size 8 --per_device_eval_batch_size 8  --include_inputs_for_metrics --fp16 --dataset_name xsum --dataset_config_name 3.0.0  --model_name_or_path facebook/bart-large-xsum  --do_train --do_eval  --max_source_length 512 --max_target_length 64 --overwrite_output_dir --save_strategy epoch  --evaluation_strategy steps --logging_strategy steps --logging_steps 100  --output_dir out_xsum_5000_1000_allbert_train2_512 --num_beams 6 --remove_unused_columns true --predict_with_generate --warmup_steps 100  --label_smoothing 0.1 --learning_rate 3e-5 --gradient_accumulation_step 4

#copy mech
python src/run_summarization_custom_copy.py --eval_steps 2000 --num_train_epochs 5 --max_train_samples 5000 --max_eval_samples 1000 --per_device_train_batch_size 8 --per_device_eval_batch_size 8  --include_inputs_for_metrics --fp16 --dataset_name xsum   --model_name_or_path facebook/bart-large-xsum  --do_train --do_eval  --max_source_length 512 --max_target_length 64 --overwrite_output_dir --save_strategy epoch  --evaluation_strategy steps --logging_strategy steps --logging_steps 100  --output_dir out_xsum_5000_1000_copy_train_512 --num_beams 6 --remove_unused_columns true --predict_with_generate --warmup_steps 100  --label_smoothing 0.1 --learning_rate 3e-5 --gradient_accumulation_step 4
# drouput 0.3
python src/run_summarization_custom_copy.py --eval_steps 1000 --num_train_epochs 5 --max_train_samples 20000 --max_eval_samples 1000 --per_device_train_batch_size 8 --per_device_eval_batch_size 8  --include_inputs_for_metrics --fp16 --dataset_name xsum   --model_name_or_path facebook/bart-large-xsum  --do_train --do_eval  --max_source_length 512 --max_target_length 64 --overwrite_output_dir --save_strategy epoch  --evaluation_strategy steps --logging_strategy steps --logging_steps 100  --output_dir out_xsum_20000_1000_copy_train_512 --num_beams 6 --remove_unused_columns true --predict_with_generate --warmup_steps 100  --label_smoothing 0.1 --learning_rate 3e-5 --gradient_accumulation_step 4
python src/run_summarization_custom_copy.py --eval_steps 2000 --num_train_epochs 5 --max_train_samples 20000 --max_eval_samples 1000 --per_device_train_batch_size 8 --per_device_eval_batch_size 8  --include_inputs_for_metrics --fp16 --dataset_name xsum   --model_name_or_path facebook/bart-large-xsum  --do_train --do_eval  --max_source_length 512 --max_target_length 64 --overwrite_output_dir --save_strategy epoch  --evaluation_strategy steps --logging_strategy steps --logging_steps 100  --output_dir out_xsum_20000_1000_copy_train_fine_tune_all_512 --num_beams 6 --remove_unused_columns true --predict_with_generate --warmup_steps 100  --label_smoothing 0.1 --learning_rate 3e-5 --gradient_accumulation_step 4
python src/run_summarization_custom_copy.py --eval_steps 2000 --num_train_epochs 5  --max_eval_samples 1000 --per_device_train_batch_size 8 --per_device_eval_batch_size 8  --include_inputs_for_metrics --fp16 --dataset_name xsum   --model_name_or_path facebook/bart-large-xsum  --do_train --do_eval  --max_source_length 512 --max_target_length 64 --overwrite_output_dir --save_strategy epoch  --evaluation_strategy steps --logging_strategy steps --logging_steps 100  --output_dir out_xsum_full_1000_copy_train_512 --num_beams 6 --remove_unused_columns true --predict_with_generate --warmup_steps 500  --label_smoothing 0.1 --learning_rate 3e-5 --gradient_accumulation_step 4

# dropout 0.5
python src/run_summarization_custom_copy.py --eval_steps 1000 --num_train_epochs 5 --max_train_samples 20000 --max_eval_samples 1000 --per_device_train_batch_size 8 --per_device_eval_batch_size 8  --include_inputs_for_metrics --fp16 --dataset_name xsum   --model_name_or_path facebook/bart-large-xsum  --do_train --do_eval  --max_source_length 512 --max_target_length 64 --overwrite_output_dir --save_strategy epoch  --evaluation_strategy steps --logging_strategy steps --logging_steps 100  --output_dir out_xsum_20000_1000_copy_dropput_0.5_train_512 --num_beams 6 --remove_unused_columns true --predict_with_generate --warmup_steps 100  --label_smoothing 0.1 --learning_rate 3e-5 --gradient_accumulation_step 4

# dropout 0.3 # pgen=0.5 pcopy=+0.4
python src/run_summarization_custom_copy.py --eval_steps 1000 --num_train_epochs 5 --max_train_samples 5000 --max_eval_samples 1000 --per_device_train_batch_size 8 --per_device_eval_batch_size 8  --include_inputs_for_metrics --fp16 --dataset_name xsum   --model_name_or_path facebook/bart-large-xsum  --do_train --do_eval  --max_source_length 512 --max_target_length 64 --overwrite_output_dir --save_strategy epoch  --evaluation_strategy steps --logging_strategy steps --logging_steps 100  --output_dir out_xsum_5000_1000_biased_copy_train_512 --num_beams 6 --remove_unused_columns true --predict_with_generate --warmup_steps 100  --label_smoothing 0.1 --learning_rate 3e-5 --gradient_accumulation_step 4
# dropout 0.3 # pgen=0.2 pcopy=+0.9
python src/run_summarization_custom_copy.py --eval_steps 1000 --num_train_epochs 5 --max_train_samples 5000 --max_eval_samples 1000 --per_device_train_batch_size 8 --per_device_eval_batch_size 8  --include_inputs_for_metrics --fp16 --dataset_name xsum   --model_name_or_path facebook/bart-large-xsum  --do_train --do_eval  --max_source_length 512 --max_target_length 64 --overwrite_output_dir --save_strategy epoch  --evaluation_strategy steps --logging_strategy steps --logging_steps 100  --output_dir out_xsum_5000_1000_biased_2_copy_train_512 --num_beams 6 --remove_unused_columns true --predict_with_generate --warmup_steps 100  --label_smoothing 0.1 --learning_rate 3e-5 --gradient_accumulation_step 4

# dropout 0.3 # encoder_last finetune
python src/run_summarization_custom_copy.py --eval_steps 1000 --num_train_epochs 5 --max_train_samples 20000 --max_eval_samples 1000 --per_device_train_batch_size 8 --per_device_eval_batch_size 8  --include_inputs_for_metrics --fp16 --dataset_name xsum   --model_name_or_path facebook/bart-large-xsum  --do_train --do_eval  --max_source_length 512 --max_target_length 64 --overwrite_output_dir --save_strategy epoch  --evaluation_strategy steps --logging_strategy steps --logging_steps 100  --output_dir out_xsum_20000_1000_encoder_last_copy_train_512 --num_beams 6 --remove_unused_columns true --predict_with_generate --warmup_steps 100  --label_smoothing 0.1 --learning_rate 3e-5 --gradient_accumulation_step 4
# dropout 0.3 #encoder_decoder_last_layer
python src/run_summarization_custom_copy.py --eval_steps 1000 --num_train_epochs 5 --max_train_samples 20000 --max_eval_samples 1000 --per_device_train_batch_size 8 --per_device_eval_batch_size 8  --include_inputs_for_metrics --fp16 --dataset_name xsum   --model_name_or_path facebook/bart-large-xsum  --do_train --do_eval  --max_source_length 512 --max_target_length 64 --overwrite_output_dir --save_strategy epoch  --evaluation_strategy steps --logging_strategy steps --logging_steps 100  --output_dir out_xsum_20000_1000_enc_dec_last_copy_train_512 --num_beams 6 --remove_unused_columns true --predict_with_generate --warmup_steps 100  --label_smoothing 0.1 --learning_rate 3e-5 --gradient_accumulation_step 4
# dropout 0.3 # encoder_full finetune
python src/run_summarization_custom_copy.py --eval_steps 1000 --num_train_epochs 5 --max_train_samples 20000 --max_eval_samples 1000 --per_device_train_batch_size 8 --per_device_eval_batch_size 8  --include_inputs_for_metrics --fp16 --dataset_name xsum   --model_name_or_path facebook/bart-large-xsum  --do_train --do_eval  --max_source_length 512 --max_target_length 64 --overwrite_output_dir --save_strategy epoch  --evaluation_strategy steps --logging_strategy steps --logging_steps 100  --output_dir out_xsum_20000_1000_encoder_full_copy_train_512 --num_beams 6 --remove_unused_columns true --predict_with_generate --warmup_steps 100  --label_smoothing 0.1 --learning_rate 3e-5 --gradient_accumulation_step 4


# NER_MLM 0.5
python src/run_summarization_custom.py --ner_mlm_prob 1 --ner_mlm True --eval_steps 1000 --num_train_epochs 7 --max_train_samples 200 --max_eval_samples 200 --per_device_train_batch_size 8 --per_device_eval_batch_size 8  --include_inputs_for_metrics --fp16 --dataset_name xsum   --model_name_or_path facebook/bart-large-xsum  --do_train --do_eval  --max_source_length 512 --max_target_length 64 --overwrite_output_dir --save_strategy epoch  --evaluation_strategy steps --logging_strategy steps --logging_steps 100  --output_dir out_xsum_200_200_ner_train_512 --num_beams 6 --remove_unused_columns true --predict_with_generate --warmup_steps 100  --label_smoothing 0.1 --learning_rate 3e-5 --gradient_accumulation_step 4

# ner mlm 1 self xsum pretrain on targets
python src/run_summarization_custom.py --ner_mlm_prob 1 --ner_mlm True --eval_steps 1000 --num_train_epochs 5 --max_train_samples 20000 --max_eval_samples 1000 --per_device_train_batch_size 8 --per_device_eval_batch_size 8  --include_inputs_for_metrics --fp16 --dataset_name xsum   --model_name_or_path facebook/bart-large-xsum  --do_train --do_eval  --max_source_length 512 --max_target_length 64 --overwrite_output_dir --save_strategy epoch  --evaluation_strategy steps --logging_strategy steps --logging_steps 100  --output_dir out_xsum_20000_1000_ner_1_train_512 --num_beams 6 --remove_unused_columns true --predict_with_generate --warmup_steps 100  --label_smoothing 0.1 --learning_rate 3e-5 --gradient_accumulation_step 4
# ner mlm 9.5 self xsum pretrain on targets
python src/run_summarization_custom.py --ner_mlm_prob 0.5 --ner_mlm True --eval_steps 1000 --num_train_epochs 5 --max_train_samples 20000 --max_eval_samples 1000 --per_device_train_batch_size 8 --per_device_eval_batch_size 8  --include_inputs_for_metrics --fp16 --dataset_name xsum   --model_name_or_path facebook/bart-large-xsum  --do_train --do_eval  --max_source_length 512 --max_target_length 64 --overwrite_output_dir --save_strategy epoch  --evaluation_strategy steps --logging_strategy steps --logging_steps 100  --output_dir out_xsum_20000_1000_ner_0.5_train_512 --num_beams 6 --remove_unused_columns true --predict_with_generate --warmup_steps 100  --label_smoothing 0.1 --learning_rate 3e-5 --gradient_accumulation_step 4
# ner mlm self xsum use pretrained mlm 1
python src/run_summarization_custom.py --resume_from_checkpoint out_xsum_20000_1000_ner_1_train_512/checkpoint-3125  --eval_steps 1000 --num_train_epochs 7 --max_train_samples 20000 --max_eval_samples 1000 --per_device_train_batch_size 8 --per_device_eval_batch_size 8  --include_inputs_for_metrics --fp16 --dataset_name xsum   --model_name_or_path facebook/bart-large-xsum  --do_train --do_eval  --max_source_length 512 --max_target_length 64 --overwrite_output_dir --save_strategy epoch  --evaluation_strategy steps --logging_strategy steps --logging_steps 100  --output_dir out_xsum_20000_1000_ner_pretrained_1_train_512 --num_beams 6 --remove_unused_columns true --predict_with_generate --warmup_steps 100  --label_smoothing 0.1 --learning_rate 3e-5 --gradient_accumulation_step 4
# ner mlm self xsum use pretrained mlm 0.5
python src/run_summarization_custom.py --ner_mlm_prob 0.5 --ner_mlm True --eval_steps 1000 --num_train_epochs 2 --max_train_samples 20000 --max_eval_samples 1000 --per_device_train_batch_size 8 --per_device_eval_batch_size 8  --include_inputs_for_metrics --fp16 --dataset_name xsum   --model_name_or_path facebook/bart-large-xsum  --do_train --do_eval  --max_source_length 512 --max_target_length 64 --overwrite_output_dir --save_strategy epoch  --evaluation_strategy steps --logging_strategy steps --logging_steps 100  --output_dir out_xsum_20000_1000_ner_0.5_train_512 --num_beams 6 --remove_unused_columns true --predict_with_generate --warmup_steps 100  --label_smoothing 0.1 --learning_rate 3e-5 --gradient_accumulation_step 4

# ner mlm 0.5 cnn pretrain => xsum train mlm 0.5
python src/run_summarization_custom.py --ner_mlm_prob 0.5 --ner_mlm True  --num_train_epochs 1 --preprocessing_num_workers 8 --per_device_train_batch_size 8 --per_device_eval_batch_size 8  --include_inputs_for_metrics --fp16 --dataset_name cnn_dailymail --dataset_config_name 3.0.0    --model_name_or_path facebook/bart-large-xsum  --do_train  --max_source_length 512 --max_target_length 64 --overwrite_output_dir --save_strategy epoch  --evaluation_strategy no --logging_strategy steps --logging_steps 100  --output_dir out_cnndaily_full_1000_ner_0.5_train_512 --num_beams 6 --remove_unused_columns true --predict_with_generate --warmup_steps 100  --label_smoothing 0.1 --learning_rate 3e-5 --gradient_accumulation_step 4
python src/run_summarization_custom.py --eval_steps 1000 --num_train_epochs 1 --preprocessing_num_workers 12 --max_eval_samples 1000 --per_device_train_batch_size 8 --per_device_eval_batch_size 8  --include_inputs_for_metrics --fp16 --dataset_name xsum   --model_name_or_path out_cnndaily_full_1000_ner_0.5_train_512  --do_train --do_eval  --max_source_length 512 --max_target_length 64 --overwrite_output_dir --save_strategy epoch  --evaluation_strategy steps --logging_strategy steps --logging_steps 100  --output_dir out_xsum_full_1000_ner_pretrained_cnndaily_0.5_train_512 --num_beams 6 --remove_unused_columns true --predict_with_generate --warmup_steps 500  --label_smoothing 0.1 --learning_rate 3e-5 --gradient_accumulation_step 4

# ner mlm 0.5 xsum pretrain => cnn train mlm 0.5
python src/run_summarization_custom.py --ner_mlm_prob 0.5 --ner_mlm True  --num_train_epochs 1 --preprocessing_num_workers 8 --per_device_train_batch_size 8 --per_device_eval_batch_size 8  --include_inputs_for_metrics --fp16 --dataset_name xsum --dataset_config_name 3.0.0    --model_name_or_path facebook/bart-large-xsum  --do_train  --max_source_length 512 --max_target_length 64 --overwrite_output_dir --save_strategy epoch  --evaluation_strategy no --logging_strategy steps --logging_steps 100  --output_dir out_xsum_full_1000_ner_0.5_train_512 --num_beams 6 --remove_unused_columns true --predict_with_generate --warmup_steps 100  --label_smoothing 0.1 --learning_rate 3e-5 --gradient_accumulation_step 4
python src/run_summarization_custom.py --eval_steps 1000 --num_train_epochs 2 --max_eval_samples 1000 --per_device_train_batch_size 8 --per_device_eval_batch_size 8  --include_inputs_for_metrics --fp16 --dataset_name xsum   --model_name_or_path facebook/bart-large-xsum  --do_train --do_eval  --max_source_length 512 --max_target_length 64 --overwrite_output_dir --save_strategy epoch  --evaluation_strategy steps --logging_strategy steps --logging_steps 100  --output_dir out_cnn_full_1000_ner_pretrained_newsroom_0.5_train_512 --num_beams 6 --remove_unused_columns true --predict_with_generate --warmup_steps 100  --label_smoothing 0.1 --learning_rate 3e-5 --gradient_accumulation_step 4

# eval mlm
python src/run_summarization_custom.py --max_eval_samples 1000 --per_device_eval_batch_size 8  --include_inputs_for_metrics --fp16 --dataset_name xsum   --model_name_or_path ./out_xsum_full_1000_ner_0.5_train_512  --do_eval  --max_source_length 512 --max_target_length 64 --overwrite_output_dir --save_strategy no  --output_dir out_xsum_full_1000_ner_0.5_eval_512 --num_beams 6 --remove_unused_columns true --predict_with_generate  --label_smoothing 0.1 --learning_rate 3e-5 --gradient_accumulation_step 4
python src/run_summarization_custom.py --max_eval_samples 1000 --per_device_eval_batch_size 8  --include_inputs_for_metrics --fp16 --dataset_name xsum   --model_name_or_path ./out_cnndaily_full_1000_ner_0.5_train_512  --do_eval  --max_source_length 512 --max_target_length 64 --overwrite_output_dir --save_strategy no  --output_dir out_cnndaily_full_1000_ner_0.5_train_512_eval --num_beams 6 --remove_unused_columns true --predict_with_generate  --label_smoothing 0.1 --learning_rate 3e-5 --gradient_accumulation_step 4


# wikipedia mlm xsum
python src/run_mlm_custom.py --resume_from_checkpoint out_wikipedia_xsum_mlm_train_512/checkpoint-4000 --dataset_name wikipedia --ner_mlm True --num_train_epochs 1 --preprocessing_num_workers 8 --per_device_train_batch_size 64 --include_inputs_for_metrics --fp16 --model_name_or_path facebook/bart-large-xsum  --do_train  --max_source_length 512 --max_target_length 512 --overwrite_output_dir --save_strategy steps --save_steps 2000  --evaluation_strategy no --logging_strategy steps --logging_steps 100  --output_dir out_wikipedia_xsum_mlm_train_512 --num_beams 6 --remove_unused_columns true --predict_with_generate --warmup_steps 500  --label_smoothing 0.1 --learning_rate 3e-5 --gradient_accumulation_step 4
python src/run_summarization_custom.py --eval_steps 1000 --num_train_epochs 1 --preprocessing_num_workers 12 --max_eval_samples 1000 --per_device_train_batch_size 8 --per_device_eval_batch_size 8  --include_inputs_for_metrics --fp16 --dataset_name xsum   --model_name_or_path out_cnndaily_full_1000_ner_0.5_train_512  --do_train --do_eval  --max_source_length 512 --max_target_length 64 --overwrite_output_dir --save_strategy epoch  --evaluation_strategy steps --logging_strategy steps --logging_steps 100  --output_dir out_xsum_full_1000_ner_pretrained_cnndaily_0.5_train_512 --num_beams 6 --remove_unused_columns true --predict_with_generate --warmup_steps 500  --label_smoothing 0.1 --learning_rate 3e-5 --gradient_accumulation_step 4

# wikipedia mlm cnn
python src/run_mlm_custom.py --dataset_name wikipedia --ner_mlm True --max_train_samples 400000 --num_train_epochs 1 --preprocessing_num_workers 8 --per_device_train_batch_size 64 --fp16 --model_name_or_path facebook/bart-large-xsum  --do_train  --max_source_length 512 --max_target_length 512 --overwrite_output_dir --save_strategy steps --save_steps 400  --evaluation_strategy no --logging_strategy steps --logging_steps 100  --output_dir out_wikipedia_cnn_mlm_train_512 --num_beams 6 --remove_unused_columns true --warmup_steps 500  --label_smoothing 0.1 --learning_rate 3e-5 --gradient_accumulation_step 4


## MLM PRETRAIN
# wikipedia mlm xsum
python src/run_mlm_custom.py --do_eval --dataset_name wikipedia --ner_mlm True --num_train_epochs 1 --preprocessing_num_workers 8  --per_device_eval_batch_size 64 --per_device_train_batch_size 64 --fp16 --model_name_or_path facebook/bart-large-xsum  --do_train  --max_source_length 1024 --max_target_length 1024 --save_strategy steps --save_steps 2000  --evaluation_strategy steps --eval_steps 4000 --logging_strategy steps --logging_steps 100  --output_dir out_wikipedia_xsum_mlm_train_1024 --num_beams 6 --remove_unused_columns true --warmup_steps 500  --label_smoothing 0.1 --learning_rate 3e-5 --gradient_accumulation_step 4

# resume from checkpoint with different batch size due to memory issue (gradient_step 8, batch_size 32 = 256 sample/step)
python src/run_mlm_custom.py  --resume_from_checkpoint out_wikipedia_xsum_mlm_train_1024/checkpoint-12000 --do_eval --dataset_name wikipedia --ner_mlm True --num_train_epochs 1 --preprocessing_num_workers 8  --per_device_eval_batch_size 64 --per_device_train_batch_size 32 --fp16 --model_name_or_path facebook/bart-large-xsum  --do_train  --max_source_length 1024 --max_target_length 1024 --save_strategy steps --save_steps 2000  --evaluation_strategy steps --eval_steps 4000 --logging_strategy steps --logging_steps 100  --output_dir out_wikipedia_xsum_mlm_train_1024 --num_beams 6 --remove_unused_columns true --warmup_steps 500  --label_smoothing 0.1 --learning_rate 3e-5 --gradient_accumulation_step 8

#python src/run_summarization_custom.py --eval_steps 1000 --num_train_epochs 1 --preprocessing_num_workers 12 --max_eval_samples 1000  --per_device_eval_batch_size 64  --include_inputs_for_metrics --fp16 --dataset_name xsum   --model_name_or_path out_cnndaily_full_1000_ner_0.5_train_512  --do_train --do_eval  --max_source_length 512 --max_target_length 64 --overwrite_output_dir --save_strategy epoch  --evaluation_strategy steps --logging_strategy steps --logging_steps 100  --output_dir out_xsum_full_1000_ner_pretrained_cnndaily_0.5_train_512 --num_beams 6 --remove_unused_columns true --predict_with_generate --warmup_steps 500  --label_smoothing 0.1 --learning_rate 3e-5 --gradient_accumulation_step 4

# xsum mlm pretrain
python src/run_mlm_custom.py --do_eval --dataset_name xsum --ner_mlm True --num_train_epochs 1 --preprocessing_num_workers 8  --per_device_eval_batch_size 4 --per_device_train_batch_size 4 --fp16 --model_name_or_path facebook/bart-large-xsum  --do_train  --max_source_length 512 --max_target_length 512 --save_strategy steps --save_steps 1500  --evaluation_strategy steps --eval_steps 1500 --logging_strategy steps --logging_steps 100  --output_dir out_xsum_xsum_mlm_train_1024 --num_beams 6 --remove_unused_columns true --warmup_steps 500  --label_smoothing 0.1 --learning_rate 3e-5 --gradient_accumulation_step 8
# xsum summarization from xsum mlm pretrain
python src/run_summarization_custom.py  --num_train_epochs 1 --do_train --per_device_train_batch_size 32 --per_device_eval_batch_size 32 --include_inputs_for_metrics --fp16 --dataset_name xsum --preprocessing_num_workers 8   --model_name_or_path out_xsum_xsum_mlm_train_1024  --do_eval  --max_source_length 512 --max_target_length 64 --save_strategy steps --save_steps 2000  --output_dir out_xsum_xsum_mlm_pretrain_train_512 --num_beams 6 --remove_unused_columns true --predict_with_generate  --label_smoothing 0.1 --learning_rate 2e-5 --gradient_accumulation_step 1 --warmup_steps 2000 --evaluation_strategy steps --eval_steps 2000 

python src/run_summarization_custom.py --resume_from_checkpoint out_xsum_xsum_mlm_pretrain_train_512  --num_train_epochs 2 --do_train --per_device_train_batch_size 32 --per_device_eval_batch_size 32 --include_inputs_for_metrics --fp16 --dataset_name xsum --preprocessing_num_workers 8   --model_name_or_path out_xsum_xsum_mlm_train_1024  --do_eval  --max_source_length 512 --max_target_length 64 --save_strategy steps --save_steps 2000  --output_dir out_xsum_xsum_mlm_pretrain_train_512 --num_beams 6 --remove_unused_columns true --predict_with_generate  --label_smoothing 0.1 --learning_rate 2e-5 --gradient_accumulation_step 1 --warmup_steps 2000 --evaluation_strategy steps --eval_steps 2000 
# xsum summarization from xsum mlm pretrain 8batch
python src/run_mlm_custom.py  --num_train_epochs 5 --do_eval --dataset_name xsum --ner_mlm True --preprocessing_num_workers 8  --per_device_eval_batch_size 8 --per_device_train_batch_size 8 --fp16 --model_name_or_path facebook/bart-large-xsum  --do_train  --max_source_length 512 --max_target_length 512 --save_strategy steps --save_steps 1500  --evaluation_strategy steps --eval_steps 1500 --logging_strategy steps --logging_steps 100  --output_dir out_xsum_xsum_mlm_train_512 --remove_unused_columns true --warmup_steps 1000 --learning_rate 2e-5 --gradient_accumulation_step 1


# xsum dataset

# 512-64 8b
python src/run_summarization_custom.py  --num_train_epochs 1 --do_train --per_device_train_batch_size 8 --fp16 --dataset_name xsum --preprocessing_num_workers 8   --model_name_or_path facebook/bart-large  --max_source_length 512 --max_target_length 64  --output_dir out_xsum_final_bart_train_512 --remove_unused_columns true --learning_rate 2e-5 --gradient_accumulation_step 1 --warmup_steps 1000 --save_strategy epoch

# 512-64 8b pretrained custom mlm xsum pretrained
python src/run_summarization_custom.py  --num_train_epochs 1 --do_train --per_device_train_batch_size 8 --fp16 --dataset_name xsum --preprocessing_num_workers 8   --model_name_or_path out_xsum_xsum_mlm_train_512/checkpoint-34500  --max_source_length 512 --max_target_length 64  --output_dir out_xsum_final_pretrain_xsuom_bart_train_512 --remove_unused_columns true --learning_rate 2e-5 --gradient_accumulation_step 1 --warmup_steps 1000 --save_strategy epoch

# cnn dataset

# 512-64 8b
python src/run_summarization_custom.py  --num_train_epochs 1 --do_train --per_device_train_batch_size 8 --fp16 --dataset_name cnn_dailymail --dataset_config_name 3.0.0 --preprocessing_num_workers 8   --model_name_or_path facebook/bart-large  --max_source_length 512 --max_target_length 64  --output_dir out_cnn_final_bart_train_512 --remove_unused_columns true --learning_rate 2e-5 --gradient_accumulation_step 1 --warmup_steps 1000 --save_strategy epoch

# 512-64 8b pretrained custom mlm xsum pretrained
python src/run_summarization_custom.py  --num_train_epochs 1 --do_train --per_device_train_batch_size 8 --fp16 --dataset_name cnn_dailymail --dataset_config_name 3.0.0 --preprocessing_num_workers 8   --model_name_or_path out_xsum_xsum_mlm_train_512/checkpoint-34500  --max_source_length 512 --max_target_length 64  --output_dir out_cnn_final_pretrain_xsum_bart_train_512 --remove_unused_columns true --learning_rate 2e-5 --gradient_accumulation_step 1 --warmup_steps 1000 --save_strategy epoch

# 1024-128 8b
python src/run_summarization_custom.py  --num_train_epochs 1 --do_train --per_device_train_batch_size 8 --fp16 --dataset_name cnn_dailymail --dataset_config_name 3.0.0 --preprocessing_num_workers 8   --model_name_or_path facebook/bart-large  --max_source_length 1024 --max_target_length 128  --output_dir out_cnn_final_bart_train_512 --remove_unused_columns true --learning_rate 2e-5 --gradient_accumulation_step 1 --warmup_steps 1000 --save_strategy epoch

# 1024-128 8b pretrained custom mlm xsum pretrained
python src/run_summarization_custom.py  --num_train_epochs 1 --do_train --per_device_train_batch_size 8 --fp16 --dataset_name cnn_dailymail --dataset_config_name 3.0.0 --preprocessing_num_workers 8   --model_name_or_path out_xsum_xsum_mlm_train_512/checkpoint-34500  --max_source_length 1024 --max_target_length 128  --output_dir out_cnn_final_pretrain_xsum_bart_train_512 --remove_unused_columns true --learning_rate 2e-5 --gradient_accumulation_step 1 --warmup_steps 1000 --save_strategy epoch



#archive dataset

# fine-tune bart-large vs fine-tune xsum-mlm-512

# bs 8
python src/run_summarization_custom.py  --num_train_epochs 1 --do_train --per_device_train_batch_size 8 --per_device_eval_batch_size 8 --include_inputs_for_metrics --fp16 --dataset_name ccdv/arxiv-summarization --preprocessing_num_workers 8   --model_name_or_path facebook/bart-large  --do_eval  --max_source_length 512 --max_target_length 64 --save_strategy steps --save_steps 2000  --output_dir out_arxiv_bart_train_512 --num_beams 6 --remove_unused_columns true --predict_with_generate --learning_rate 2e-5 --gradient_accumulation_step 1 --warmup_steps 500 
python src/run_summarization_custom.py  --num_train_epochs 2 --do_train --per_device_train_batch_size 8 --per_device_eval_batch_size 8 --include_inputs_for_metrics --fp16 --dataset_name ccdv/arxiv-summarization --preprocessing_num_workers 8   --model_name_or_path facebook/bart-large  --do_eval  --max_source_length 512 --max_target_length 64 --save_strategy steps --save_steps 20000  --output_dir out_arxiv_bart_train_512_b8_2epoch --num_beams 6 --remove_unused_columns true --predict_with_generate --learning_rate 2e-5 --gradient_accumulation_step 1 --warmup_steps 1000 --resume_from_checkpoint  out_arxiv_bart_train_512

#label smoothing 
python src/run_summarization_custom.py  --num_train_epochs 1 --do_train --per_device_train_batch_size 8 --per_device_eval_batch_size 8 --include_inputs_for_metrics --fp16 --dataset_name ccdv/arxiv-summarization --preprocessing_num_workers 8   --model_name_or_path facebook/bart-large  --do_eval  --max_source_length 512 --max_target_length 64 --save_strategy steps --save_steps 5000  --output_dir out_arxiv_bart_train_512_smoothing0.1 --num_beams 6 --remove_unused_columns true --predict_with_generate --learning_rate 2e-5 --gradient_accumulation_step 1 --warmup_steps 500 --label_smoothing 0.1

# bs 1
python src/run_summarization_custom.py  --num_train_epochs 1 --do_train --per_device_train_batch_size 1 --per_device_eval_batch_size 8 --include_inputs_for_metrics --fp16 --dataset_name ccdv/arxiv-summarization --preprocessing_num_workers 8   --model_name_or_path facebook/bart-large  --do_eval  --max_source_length 512 --max_target_length 64 --save_strategy steps --save_steps 2000  --output_dir out_arxiv_bart_train_512_b1 --num_beams 6 --remove_unused_columns true --predict_with_generate --learning_rate 2e-5 --gradient_accumulation_step 1 --warmup_steps 5000 


# bs 8
python src/run_summarization_custom.py  --num_train_epochs 1 --do_train --per_device_train_batch_size 8 --per_device_eval_batch_size 8 --include_inputs_for_metrics --fp16 --dataset_name ccdv/arxiv-summarization --preprocessing_num_workers 8   --model_name_or_path out_xsum_xsum_mlm_train_512/checkpoint-34500  --do_eval  --max_source_length 512 --max_target_length 64 --save_strategy steps --save_steps 2000  --output_dir out_arxiv_xsun_pretrain_train_512 --num_beams 6 --remove_unused_columns true --predict_with_generate --learning_rate 2e-5 --gradient_accumulation_step 1 --warmup_steps 500 --evaluation_strategy epoch
# bs 1
python src/run_summarization_custom.py  --num_train_epochs 1 --do_train --per_device_train_batch_size 1 --per_device_eval_batch_size 8 --include_inputs_for_metrics --fp16 --dataset_name ccdv/arxiv-summarization --preprocessing_num_workers 8   --model_name_or_path out_xsum_xsum_mlm_train_512/checkpoint-34500  --do_eval  --max_source_length 512 --max_target_length 64 --save_strategy steps --save_steps 20000  --output_dir out_arxiv_xsun_pretrain_train_512_b1 --num_beams 6 --remove_unused_columns true --predict_with_generate --learning_rate 2e-5 --gradient_accumulation_step 1 --warmup_steps 5000 


# b8 bart-large
python src/run_summarization_custom.py  --num_train_epochs 1 --do_train --per_device_train_batch_size 8 --include_inputs_for_metrics --fp16 --dataset_name ccdv/arxiv-summarization --preprocessing_num_workers 8   --model_name_or_path facebook/bart-large  --max_source_length 512 --max_target_length 128 --save_strategy steps --save_steps 10000  --output_dir out_arxiv_bart_train_512_128 --num_beams 6 --remove_unused_columns true --learning_rate 2e-5 --gradient_accumulation_step 1 --warmup_steps 5000 

# arxiv mlm

python src/run_mlm_custom.py --overwrite_output_dir --text_column masked_document --summary_column mlm_label --num_train_epochs 1 --do_eval --dataset_name arxiv --ner_mlm True --preprocessing_num_workers 8  --per_device_eval_batch_size 4 --per_device_train_batch_size 4 --fp16 --model_name_or_path facebook/bart-large  --do_train  --max_source_length 512 --max_target_length 512 --save_strategy steps --save_steps 1500  --evaluation_strategy epoch --logging_strategy steps --logging_steps 500  --output_dir out_arxiv_mlm_train_512 --remove_unused_columns true --warmup_steps 1000 --learning_rate 2e-5 --gradient_accumulation_step 1 --label_smoothing 0.1


python src/run_mlm_custom.py --overwrite_output_dir --text_column masked_document --summary_column mlm_label --num_train_epochs 1 --do_eval --dataset_name arxiv-1024 --ner_mlm True --preprocessing_num_workers 8  --per_device_eval_batch_size 1 --per_device_train_batch_size 1 --fp16 --model_name_or_path facebook/bart-large  --do_train  --max_source_length 1024 --max_target_length 1024 --save_strategy steps --save_steps 10000  --evaluation_strategy epoch --logging_strategy steps --logging_steps 500  --output_dir out_arxiv_mlm_train_1024 --remove_unused_columns true --warmup_steps 5000 --learning_rate 2e-5 --gradient_accumulation_step 1 --label_smoothing 0.1

python src/run_mlm_custom.py --text_column masked_document --summary_column mlm_label --do_eval --dataset_name arxiv-1024 --ner_mlm True --preprocessing_num_workers 8  --per_device_eval_batch_size 1 --fp16 --model_name_or_path out_arxiv_mlm_train_1024/checkpoint-80000  --max_source_length 1024 --max_target_length 1024 --output_dir out_arxiv_mlm_train_1024_eval --remove_unused_columns true



python src/run_mlm_custom.py --resume_from_checkpoint out_arxiv_mlm_train_1024/checkpoint-80000 --text_column masked_document --summary_column mlm_label --num_train_epochs 2 --do_eval --dataset_name arxiv_1024 --ner_mlm True --preprocessing_num_workers 8  --per_device_eval_batch_size 1 --per_device_train_batch_size 1 --fp16 --model_name_or_path facebook/bart-large  --do_train  --max_source_length 1024 --max_target_length 1024 --save_strategy steps --save_steps 25000  --evaluation_strategy epoch --output_dir out_arxiv_mlm_train_1024 --remove_unused_columns true --warmup_steps 30000 --learning_rate 2e-5 --gradient_accumulation_step 1

python src/run_mlm_custom.py --resume_from_checkpoint out_arxiv_mlm_train_1024/checkpoint-125000 --text_column masked_document --summary_column mlm_label --num_train_epochs 2 --do_eval --dataset_name arxiv_1024 --ner_mlm True --preprocessing_num_workers 8  --per_device_eval_batch_size 1 --per_device_train_batch_size 1 --fp16 --model_name_or_path facebook/bart-large  --do_train  --max_source_length 1024 --max_target_length 1024 --save_strategy steps --save_steps 25000  --evaluation_strategy epoch --output_dir out_arxiv_mlm_train_1024 --remove_unused_columns true --warmup_steps 30000 --learning_rate 2e-5 --gradient_accumulation_step 1

# 0.3 1024
python src/run_mlm_custom.py --num_train_epochs 5  --evaluation_strategy epoch --do_train  --max_train_samples 2000 --max_eval_samples 1000   --text_column masked_document --summary_column mlm_label --do_eval --dataset_name arxiv-1024 --ner_mlm True --preprocessing_num_workers 8  --per_device_train_batch_size 2 --per_device_eval_batch_size 2 --fp16 --model_name_or_path facebook/bart-large  --max_source_length 1024 --max_target_length 1024 --save_strategy steps --save_steps 5000  --output_dir out_arxiv_mlm_train_1024_0.35 --remove_unused_columns true --learning_rate 2e-5  --gradient_accumulation_step 1 --warmup_steps 5000 --label_smoothing 0.1

python src/run_mlm_custom.py --num_train_epochs 1  --evaluation_strategy epoch --do_train   --text_column masked_document --summary_column mlm_label --do_eval --dataset_name arxiv-1024 --ner_mlm True --preprocessing_num_workers 8  --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --fp16 --model_name_or_path facebook/bart-large  --max_source_length 1024 --max_target_length 1024 --save_strategy steps --save_steps 20000  --output_dir out_arxiv_mlm_train_1024_0.35 --remove_unused_columns true --learning_rate 2e-5  --gradient_accumulation_step 1 --warmup_steps 30000

python src/run_mlm_custom.py --max_train_samples 2000 --text_column masked_document --summary_column mlm_label --num_train_epochs 5 --do_eval --dataset_name arxiv --ner_mlm True --preprocessing_num_workers 8  --per_device_eval_batch_size 1 --per_device_train_batch_size 1 --fp16 --model_name_or_path facebook/bart-large  --do_train  --max_source_length 512 --max_target_length 512 --save_strategy steps --save_steps 15000  --evaluation_strategy epoch --logging_strategy steps --logging_steps 500  --output_dir out_arxiv_mlm_train_512_exp --remove_unused_columns true --warmup_steps 10000 --learning_rate 2e-5 --gradient_accumulation_step 1

python src/run_mlm_custom.py --overwrite_output_dir --max_eval_samples 1000 --max_train_samples 1000 --text_column masked_document --summary_column mlm_label --num_train_epochs 5 --do_eval --dataset_name arxiv --ner_mlm True --preprocessing_num_workers 8  --per_device_eval_batch_size 1 --per_device_train_batch_size 1 --fp16 --model_name_or_path out_arxiv_mlm_train_512  --do_train  --max_source_length 512 --max_target_length 512 --save_strategy steps --save_steps 15000  --evaluation_strategy epoch --logging_strategy steps --logging_steps 500  --output_dir out_arxiv_mlm_train_512_exp --remove_unused_columns true --warmup_steps 10000 --learning_rate 2e-5 --gradient_accumulation_step 1


python src/run_mlm_custom.py --overwrite_output_dir --text_column masked_document --summary_column mlm_label --do_eval --dataset_name arxiv --ner_mlm True --preprocessing_num_workers 8  --per_device_eval_batch_size 1 --fp16 --model_name_or_path out_arxiv_mlm_train_1024/checkpoint-200000 --max_source_length 1024 --max_target_length 1024  --evaluation_strategy epoch  --output_dir out_arxiv_mlm_train_1024_exp 

# arxiv sum with mlm
# b8 label smooting
python src/run_summarization_custom.py  --num_train_epochs 1 --do_train --per_device_train_batch_size 8 --per_device_eval_batch_size 8 --include_inputs_for_metrics --fp16 --dataset_name ccdv/arxiv-summarization --preprocessing_num_workers 8   --model_name_or_path out_arxiv_mlm_train_512  --do_eval  --max_source_length 512 --max_target_length 64 --save_strategy steps --save_steps 5000  --output_dir out_arxiv_pretrainarxiv_train_512_smoothing0.1 --num_beams 6 --remove_unused_columns true --predict_with_generate --learning_rate 2e-5 --gradient_accumulation_step 1 --warmup_steps 500 --label_smoothing 0.1

# b8 512 128
python src/run_summarization_custom.py  --num_train_epochs 1 --do_train --per_device_train_batch_size 8 --fp16 --dataset_name ccdv/arxiv-summarization --preprocessing_num_workers 8   --model_name_or_path out_arxiv_mlm_train_512  --max_source_length 512 --max_target_length 128 --save_strategy steps --save_steps 10000  --output_dir out_arxiv_pretrainarxiv_train_512_128 --num_beams 6 --remove_unused_columns true --predict_with_generate --learning_rate 2e-5 --gradient_accumulation_step 1 --warmup_steps 5000 --label_smoothing 0.1


python src/run_summarization_custom.py --max_train_samples 5000  --num_train_epochs 1 --do_train --per_device_train_batch_size 8 --per_device_eval_batch_size 8 --include_inputs_for_metrics --fp16 --dataset_name ccdv/arxiv-summarization --preprocessing_num_workers 8   --model_name_or_path out_arxiv_mlm_train_512  --do_eval  --max_source_length 512 --max_target_length 100 --save_strategy steps --save_steps 5000  --output_dir out_arxiv_pretrainarxiv_train_512_smoothing0.1_5000 --num_beams 6 --remove_unused_columns true --predict_with_generate --learning_rate 2e-5 --gradient_accumulation_step 1 --warmup_steps 500 --label_smoothing 0.1


python src/run_summarization_custom.py  --num_train_epochs 1 --do_train --per_device_train_batch_size 8 --per_device_eval_batch_size 8 --include_inputs_for_metrics --fp16 --dataset_name ccdv/arxiv-summarization --preprocessing_num_workers 8   --model_name_or_path out_xsum_xsum_mlm_train_512/checkpoint-34500  --do_eval  --max_source_length 512 --max_target_length 128 --save_strategy steps --save_steps 10000  --output_dir out_arxiv_xsun_pretrain_train_512_128_b1_beams1 --num_beams 1 --remove_unused_columns true --predict_with_generate --learning_rate 2e-5 --gradient_accumulation_step 1 --warmup_steps 5000 --label_smoothing 0.1

python src/run_summarization_custom.py  --num_train_epochs 1 --do_train --per_device_train_batch_size 8 --per_device_eval_batch_size 8 --include_inputs_for_metrics --fp16 --dataset_name ccdv/arxiv-summarization --preprocessing_num_workers 8   --model_name_or_path out_xsum_xsum_mlm_train_512/checkpoint-34500  --do_eval  --max_source_length 512 --max_target_length 128 --save_strategy steps --save_steps 10000  --output_dir out_arxiv_xsun_pretrain_train_512_128_b1_beams1 --num_beams 1 --remove_unused_columns true --predict_with_generate --learning_rate 2e-5 --gradient_accumulation_step 1 --warmup_steps 5000 --label_smoothing 0.1

# 1024-128
python src/run_summarization_custom.py  --num_train_epochs 1 --do_train --per_device_train_batch_size 8 --per_device_eval_batch_size 8 --include_inputs_for_metrics --fp16 --dataset_name ccdv/arxiv-summarization --preprocessing_num_workers 8   --model_name_or_path out_xsum_xsum_mlm_train_512/checkpoint-34500  --do_eval  --max_source_length 1024 --max_target_length 128 --save_strategy steps --save_steps 10000  --output_dir out_arxiv_xsun_pretrain_train_1024_128_b8_beams6 --num_beams 6 --remove_unused_columns true --predict_with_generate --learning_rate 2e-5 --gradient_accumulation_step 1 --warmup_steps 5000 --label_smoothing 0.1

# 1024-256
python src/run_summarization_custom.py  --num_train_epochs 1 --do_train --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --include_inputs_for_metrics --fp16 --dataset_name ccdv/arxiv-summarization --preprocessing_num_workers 8   --model_name_or_path out_xsum_xsum_mlm_train_512/checkpoint-34500  --do_eval  --max_source_length 1024 --max_target_length 256 --save_strategy steps --save_steps 10000  --output_dir out_arxiv_xsun_pretrain_train_1024_2568_b8_beams6 --num_beams 6 --remove_unused_columns true --predict_with_generate --learning_rate 2e-5 --gradient_accumulation_step 8 --warmup_steps 5000 --label_smoothing 0.1


# 1024-128
python src/run_summarization_custom.py  --num_train_epochs 1 --do_train --per_device_train_batch_size 8 --per_device_eval_batch_size 8 --include_inputs_for_metrics --fp16 --dataset_name ccdv/arxiv-summarization --preprocessing_num_workers 8   --model_name_or_path out_arxiv_mlm_train_1024/checkpoint-80000  --do_eval  --max_source_length 1024 --max_target_length 128 --save_strategy steps --save_steps 10000  --output_dir out_arxiv_arxiv_pretrain_train_1024_128_b8_beams6 --num_beams 6 --remove_unused_columns true --predict_with_generate --learning_rate 2e-5 --gradient_accumulation_step 1 --warmup_steps 5000 --label_smoothing 0.1
python src/run_summarization_custom.py --resume_from_checkpoint out_arxiv_arxiv_pretrain_train_1024_128_b8_beams6/checkpoint-10000  --num_train_epochs 1 --do_train --per_device_train_batch_size 8 --per_device_eval_batch_size 8 --include_inputs_for_metrics --fp16 --dataset_name ccdv/arxiv-summarization --preprocessing_num_workers 8   --model_name_or_path out_arxiv_mlm_train_1024/checkpoint-80000  --do_eval  --max_source_length 1024 --max_target_length 128 --save_strategy steps --save_steps 10000  --output_dir out_arxiv_arxiv_pretrain_train_1024_128_b8_beams6 --num_beams 6 --remove_unused_columns true --predict_with_generate --learning_rate 2e-5 --gradient_accumulation_step 1 --warmup_steps 5000 --label_smoothing 0.1

python src/run_summarization_custom.py  --num_train_epochs 1 --do_train --per_device_train_batch_size 8 --per_device_eval_batch_size 8 --include_inputs_for_metrics --fp16 --dataset_name ccdv/arxiv-summarization --preprocessing_num_workers 8   --model_name_or_path out_arxiv_mlm_train_1024_0.35_2epoch  --do_eval  --max_source_length 1024 --max_target_length 128 --save_strategy steps --save_steps 10000  --output_dir out_arxiv_arxiv_pretrain2_train_1024_128_b8_beams6 --num_beams 6 --remove_unused_columns true --predict_with_generate --learning_rate 2e-5 --gradient_accumulation_step 1 --warmup_steps 5000 --label_smoothing 0.1

#new full_arxive pretrain 1024-128
python src/run_summarization_custom.py  --num_train_epochs 1 --do_train --per_device_train_batch_size 8 --fp16 --dataset_name ccdv/arxiv-summarization --preprocessing_num_workers 8   --model_name_or_path out_arxiv_mlm_train_1024/checkpoint-200000  --max_source_length 1024 --max_target_length 128 --save_strategy steps --save_steps 10000  --output_dir out_arxiv_full_arxiv_pretrain_train_1024_128_b8_beams6 --num_beams 6 --remove_unused_columns true --learning_rate 2e-5 --gradient_accumulation_step 1 --warmup_steps 5000 --label_smoothing 0.1
python src/run_summarization_custom.py  --num_train_epochs 1 --do_train --per_device_train_batch_size 8 --fp16 --dataset_name ccdv/arxiv-summarization --preprocessing_num_workers 8   --model_name_or_path out_arxiv_mlm_train_1024/checkpoint-200000  --max_source_length 512 --max_target_length 64 --save_strategy steps --save_steps 10000  --output_dir out_arxiv_full_arxiv_pretrain_train_512_b8_beams6 --num_beams 6 --remove_unused_columns true --learning_rate 2e-5 --gradient_accumulation_step 1 --warmup_steps 5000 --label_smoothing 0.1


# 1024-128
python src/run_summarization_custom.py  --num_train_epochs 1 --do_train --per_device_train_batch_size 8 --per_device_eval_batch_size 8 --include_inputs_for_metrics --fp16 --dataset_name ccdv/arxiv-summarization --preprocessing_num_workers 8   --model_name_or_path facebook/bart-large  --do_eval  --max_source_length 1024 --max_target_length 128 --save_strategy steps --save_steps 7500  --output_dir out_arxiv_bart_large_train_1024_128_b8_beams6 --num_beams 6 --remove_unused_columns true --predict_with_generate --learning_rate 2e-5 --gradient_accumulation_step 1 --warmup_steps 5000 --label_smoothing 0.1

# arxiv eval
python src/run_summarization_custom.py --per_device_eval_batch_size 8 --include_inputs_for_metrics --fp16 --dataset_name ccdv/arxiv-summarization --preprocessing_num_workers 8   --model_name_or_path out_arxiv_xsun_pretrain_train_1024_128_b8_beams6  --do_eval  --max_source_length 1024 --max_target_length 128  --output_dir out_arxiv_xsun_pretrain_train_1024_128_b8_eval_1beams --num_beams 1 --remove_unused_columns true --predict_with_generate 

python src/run_summarization_custom.py  --per_device_eval_batch_size 8 --include_inputs_for_metrics --fp16 --dataset_name ccdv/arxiv-summarization --preprocessing_num_workers 8   --model_name_or_path out_arxiv_xsun_pretrain_train_512_128_b1_beams1  --do_eval  --max_source_length 512 --max_target_length 128 --output_dir out_arxiv_xsun_pretrain_train_512_128_b1_beams6_eval --num_beams 6 --remove_unused_columns true --predict_with_generate

python src/run_summarization_custom.py  --per_device_eval_batch_size 8 --include_inputs_for_metrics --fp16 --dataset_name ccdv/arxiv-summarization --preprocessing_num_workers 8   --model_name_or_path out_arxiv_xsun_pretrain_train_1024_2568_b8_beams6  --do_eval  --max_source_length 1024 --max_target_length 256 --output_dir out_arxiv_xsun_pretrain_train_1024_2568_b8_beams1_eval --num_beams 1 --remove_unused_columns true --predict_with_generate

python src/run_summarization_custom.py  --per_device_eval_batch_size 8 --include_inputs_for_metrics --fp16 --dataset_name ccdv/arxiv-summarization --preprocessing_num_workers 8   --model_name_or_path out_arxiv_arxiv_pretrain_train_1024_128_b8_beams6  --do_eval  --max_source_length 1024 --max_target_length 128 --output_dir out_arxiv_arxiv_pretrain_train_1024_128_b8_beams1 --num_beams 1 --remove_unused_columns true --predict_with_generate



# wikipedia mlm cnn
python src/run_mlm_custom.py --do_eval --dataset_name wikipedia --ner_mlm True --num_train_epochs 1 --preprocessing_num_workers 8  --per_device_eval_batch_size 64 --per_device_train_batch_size 64 --fp16 --model_name_or_path facebook/bart-large-cnn  --do_train  --max_source_length 1024 --max_target_length 1024 --save_strategy steps --save_steps 2000  --evaluation_strategy steps --eval_steps 4000 --logging_strategy steps --logging_steps 100  --output_dir out_wikipedia_cnn_mlm_train_1024 --num_beams 6 --remove_unused_columns true --warmup_steps 500  --label_smoothing 0.1 --learning_rate 3e-5 --gradient_accumulation_step 4
# resume from checkpoint with different batch size due to memory issue (gradient_step 8, batch_size 32 = 256 sample/step)
python src/run_mlm_custom.py  --resume_from_checkpoint out_wikipedia_cnn_mlm_train_1024/checkpoint-12000 --do_eval --dataset_name wikipedia --ner_mlm True --num_train_epochs 1 --preprocessing_num_workers 8  --per_device_eval_batch_size 64 --per_device_train_batch_size 32 --fp16 --model_name_or_path facebook/bart-large-cnn  --do_train  --max_source_length 1024 --max_target_length 1024 --save_strategy steps --save_steps 2000  --evaluation_strategy steps --eval_steps 4000 --logging_strategy steps --logging_steps 100  --output_dir out_wikipedia_cnn_mlm_train_1024 --num_beams 6 --remove_unused_columns true --warmup_steps 500  --label_smoothing 0.1 --learning_rate 3e-5 --gradient_accumulation_step 8


# wikipedia mlm xsum train
python src/run_summarization_custom.py --do_train --per_device_eval_batch_size 32 --per_device_eval_batch_size 32 --include_inputs_for_metrics --fp16 --dataset_name xsum --preprocessing_num_workers 8   --model_name_or_path out_wikipedia_xsum_mlm_train_1024  --do_eval  --max_source_length 512 --max_target_length 64 --save_strategy no  --output_dir out_xsum_wiki_pretrain_train_512 --num_beams 6 --remove_unused_columns true --predict_with_generate  --label_smoothing 0.1 --learning_rate 3e-5 --gradient_accumulation_step 4
python src/run_summarization_custom.py  --num_train_epochs 1 --do_train --per_device_train_batch_size 32 --fp16 --dataset_name xsum  --preprocessing_num_workers 8   --model_name_or_path out_wikipedia_xsum_mlm_train_1024  --max_source_length 512 --max_target_length 64 --save_strategy steps --save_steps 2000  --output_dir out_xsum_wiki_pretrain_train_512_1epoch --num_beams 6 --remove_unused_columns true  --label_smoothing 0.1 --learning_rate 3e-5 --gradient_accumulation_step 4
python src/run_summarization_custom.py  --num_train_epochs 5 --do_eval --evaluation_strategy epoch --per_device_eval_batch_size 32 --do_train --per_device_train_batch_size 32 --fp16 --dataset_name xsum  --preprocessing_num_workers 8   --model_name_or_path out_xsum_wiki_pretrain_train_512_1epoch  --max_source_length 512 --max_target_length 64 --save_strategy epoch  --output_dir out_xsum_wiki_pretrain_train_512_5+1epoch --num_beams 6 --remove_unused_columns true  --label_smoothing 0.1 --learning_rate 3e-5 --gradient_accumulation_step 4 --predict_with_generate --include_inputs_for_metrics
#1024

python src/run_summarization_custom.py  --num_train_epochs 1 --do_eval --evaluation_strategy epoch --per_device_eval_batch_size 16 --do_train --per_device_train_batch_size 32 --fp16 --dataset_name xsum  --preprocessing_num_workers 8   --model_name_or_path out_wikipedia_xsum_mlm_train_1024  --max_source_length 800 --max_target_length 64 --save_strategy steps --save_steps 2000  --output_dir out_wikipedia_xsum_mlm_train_800_64_1epoch --num_beams 6 --remove_unused_columns true  --label_smoothing 0.1 --learning_rate 3e-5 --gradient_accumulation_step 1 --predict_with_generate --include_inputs_for_metrics

# from BART-LARGE
# xsum summarization from BART-LARGE to check train hyper
python src/run_summarization_custom.py  --num_train_epochs 1 --do_eval --evaluation_strategy epoch --per_device_eval_batch_size 16 --do_train --per_device_train_batch_size 32 --fp16 --dataset_name xsum  --preprocessing_num_workers 8   --model_name_or_path facebook/bart-large  --max_source_length 800 --max_target_length 64 --save_strategy steps --save_steps 2000  --output_dir out_bart_large_xsum_train_800_64_1epoch --num_beams 6 --remove_unused_columns true  --label_smoothing 0.1 --learning_rate 2e-5  --gradient_accumulation_step 1 --warmup_steps 2000 --predict_with_generate --include_inputs_for_metrics
python src/run_summarization_custom.py --resume_from_checkpoint out_bart_large_xsum_train_800_64_1epoch  --num_train_epochs 2 --do_eval --evaluation_strategy epoch --per_device_eval_batch_size 16 --do_train --per_device_train_batch_size 32 --fp16 --dataset_name xsum  --preprocessing_num_workers 8   --model_name_or_path facebook/bart-large  --max_source_length 800 --max_target_length 64 --save_strategy steps --save_steps 2000  --output_dir out_bart_large_xsum_train_800_64_1epoch --num_beams 6 --remove_unused_columns true  --label_smoothing 0.1 --learning_rate 2e-5  --gradient_accumulation_step 1 --warmup_steps 2000 --predict_with_generate --include_inputs_for_metrics

python src/run_summarization_custom.py  --num_train_epochs 1 --do_eval --evaluation_strategy steps --eval_steps 13000 --per_device_eval_batch_size 16 --do_train --per_device_train_batch_size 16 --fp16 --dataset_name xsum  --preprocessing_num_workers 8   --model_name_or_path facebook/bart-large  --max_source_length 512 --max_target_length 64 --save_strategy steps --save_steps 2000  --output_dir out_bart_large_xsum_train_512_64_16b_1epoch --num_beams 6 --remove_unused_columns true  --label_smoothing 0.1 --learning_rate 2e-5  --gradient_accumulation_step 1 --warmup_steps 500 --predict_with_generate --include_inputs_for_metrics
python src/run_summarization_custom.py --resume_from_checkpoint out_bart_large_xsum_train_512_64_16b_1epoch  --num_train_epochs 3 --do_eval --evaluation_strategy steps --eval_steps 13000 --per_device_eval_batch_size 16 --do_train --per_device_train_batch_size 16 --fp16 --dataset_name xsum  --preprocessing_num_workers 8   --model_name_or_path facebook/bart-large  --max_source_length 512 --max_target_length 64 --save_strategy steps --save_steps 2000  --output_dir out_bart_large_xsum_train_512_64_16b_1epoch --num_beams 6 --remove_unused_columns true  --label_smoothing 0.1 --learning_rate 2e-5  --gradient_accumulation_step 1 --warmup_steps 500 --predict_with_generate --include_inputs_for_metrics

python src/run_summarization_custom.py  --num_train_epochs 3 --do_eval --evaluation_strategy epoch --per_device_eval_batch_size 8 --do_train --per_device_train_batch_size 8 --fp16 --dataset_name xsum  --preprocessing_num_workers 8   --model_name_or_path facebook/bart-large  --max_source_length 512 --max_target_length 64 --save_strategy steps --save_steps 2000  --output_dir out_bart_large_xsum_train_512_64_8b_3epoch --num_beams 6 --remove_unused_columns true  --label_smoothing 0.1 --learning_rate 2e-5  --gradient_accumulation_step 1 --warmup_steps 1000 --predict_with_generate --include_inputs_for_metrics
python src/run_summarization_custom.py --resume_from_checkpoint out_bart_large_xsum_train_512_64_8b_3epoch  --num_train_epochs 6 --do_eval --evaluation_strategy epoch --per_device_eval_batch_size 8 --do_train --per_device_train_batch_size 8 --fp16 --dataset_name xsum  --preprocessing_num_workers 8   --model_name_or_path facebook/bart-large  --max_source_length 512 --max_target_length 64 --save_strategy epoch  --output_dir out_bart_large_xsum_train_512_64_8b_6epoch --num_beams 6 --remove_unused_columns true  --label_smoothing 0.1 --learning_rate 2e-5  --gradient_accumulation_step 1 --warmup_steps 1000 --predict_with_generate --include_inputs_for_metrics

python src/run_summarization_custom.py  --num_train_epochs 4 --do_eval --evaluation_strategy epoch --per_device_eval_batch_size 4 --do_train --per_device_train_batch_size 4 --fp16 --dataset_name xsum  --preprocessing_num_workers 8   --model_name_or_path facebook/bart-large  --max_source_length 512 --max_target_length 64 --save_strategy steps --save_steps 2000  --output_dir out_bart_large_xsum_train_512_64_4b_4epoch --num_beams 6 --remove_unused_columns true  --label_smoothing 0.1 --learning_rate 2e-5  --gradient_accumulation_step 1 --warmup_steps 1000 --predict_with_generate --include_inputs_for_metrics

# 8b with no label-smoothing
python src/run_summarization_custom.py  --num_train_epochs 5 --do_eval --evaluation_strategy epoch --per_device_eval_batch_size 8 --do_train --per_device_train_batch_size 8 --fp16 --dataset_name xsum  --preprocessing_num_workers 8   --model_name_or_path facebook/bart-large  --max_source_length 512 --max_target_length 64 --save_strategy steps --save_steps 2000  --output_dir out_bart_large_xsum_train_512_64_8b_3epoch_s0 --num_beams 6 --remove_unused_columns true --learning_rate 2e-5  --gradient_accumulation_step 1 --warmup_steps 1000 --predict_with_generate --include_inputs_for_metrics

# mlm_slg 8
python src/run_summarization_custom.py  --num_train_epochs 1 --do_train --per_device_train_batch_size 8 --fp16 --dataset_name xsum  --preprocessing_num_workers 8   --model_name_or_path facebook/bart-large-xsum  --max_source_length 512 --max_target_length 64 --save_strategy steps --save_steps 2000  --output_dir out_bart_large_xsum_train_slg_1epoch --num_beams 6 --remove_unused_columns true --learning_rate 2e-5  --gradient_accumulation_step 1 --warmup_steps 1000 --predict_with_generate --include_inputs_for_metrics
python src/run_summarization_custom.py --do_eval --evaluation_strategy steps --eval_steps 5000 --per_device_eval_batch_size 8 --num_train_epochs 1 --do_train --per_device_train_batch_size 8 --fp16 --dataset_name xsum  --preprocessing_num_workers 8   --model_name_or_path out_bart_large_xsum_train_slg_1epoch  --max_source_length 512 --max_target_length 64 --save_strategy steps --save_steps 2000  --output_dir out_bart_large_xsum_pretrain_slg_1epoch --num_beams 6 --remove_unused_columns true --learning_rate 2e-5  --gradient_accumulation_step 1 --warmup_steps 1000 --predict_with_generate --include_inputs_for_metrics
python src/run_summarization_custom.py --do_eval --per_device_eval_batch_size 16  --fp16 --dataset_name xsum  --preprocessing_num_workers 8   --model_name_or_path out_bart_large_xsum_train_slg_1epoch  --max_source_length 512 --max_target_length 64  --output_dir out_bart_large_xsum_eval_slg_1epoch --num_beams 6 --remove_unused_columns true --predict_with_generate --include_inputs_for_metrics


# wikipedia predict
python src/run_summarization_custom.py --do_predict --per_device_eval_batch_size 32 --include_inputs_for_metrics --fp16 --dataset_name xsum --preprocessing_num_workers 8   --model_name_or_path out_xsum_wiki_pretrain_train_512 --max_source_length 512 --max_target_length 64 --save_strategy no  --output_dir out_xsum_wiki_pretrain_predict_512 --num_beams 6 --remove_unused_columns true --predict_with_generate
python src/run_summarization_custom.py --do_predict --per_device_eval_batch_size 32 --include_inputs_for_metrics --fp16 --dataset_name xsum --preprocessing_num_workers 8   --model_name_or_path out_xsum_wiki_pretrain_train_512_1epoch --max_source_length 512 --max_target_length 64 --save_strategy no  --output_dir out_xsum_wiki_pretrain_predict_512_1 --num_beams 6 --remove_unused_columns true --predict_with_generate
python src/run_summarization_custom.py --do_predict --per_device_eval_batch_size 32 --include_inputs_for_metrics --fp16 --dataset_name cnn_dailymail --dataset_config_name 3.0.0 --preprocessing_num_workers 8   --model_name_or_path out_cnn_wiki_pretrain_train_512 --max_source_length 512 --max_target_length 64 --save_strategy no  --output_dir out_cnn_wiki_pretrain_predict_512_1 --num_beams 6 --remove_unused_columns true --predict_with_generate


# wikipedia mlm cnn train
python src/run_summarization_custom.py  --num_train_epochs 1 --do_train --per_device_train_batch_size 32 --fp16 --dataset_name cnn_dailymail --dataset_config_name 3.0.0  --preprocessing_num_workers 8   --model_name_or_path out_wikipedia_cnn_mlm_train_1024  --max_source_length 512 --max_target_length 64 --save_strategy steps --save_steps 2000  --output_dir out_cnn_wiki_pretrain_train_512 --num_beams 6 --remove_unused_columns true  --label_smoothing 0.1 --learning_rate 3e-5 --gradient_accumulation_step 4
python src/run_summarization_custom.py  --num_train_epochs 5 --do_eval --evaluation_strategy epoch --per_device_eval_batch_size 32 --do_train --per_device_train_batch_size 32 --fp16 --dataset_name cnn_dailymail --dataset_config_name 3.0.0  --preprocessing_num_workers 8   --model_name_or_path out_wikipedia_cnn_mlm_train_1024  --max_source_length 1024 --max_target_length 64 --save_strategy epoch  --output_dir out_cnn_wiki_pretrain_train_512_5+1epoch --num_beams 6 --remove_unused_columns true  --label_smoothing 0.1 --learning_rate 3e-5 --gradient_accumulation_step 4 --predict_with_generate --include_inputs_for_metrics

# predict_eval
python src/run_summarization_custom.py  --num_train_epochs 10 --gpu_id 0 --max_predict_samples 200 --max_eval_samples 200 --per_device_eval_batch_size 16  --include_inputs_for_metrics --fp16 --dataset_name xsum --dataset_config_name 3.0.0  --model_name_or_path facebook/bart-large-xsum  --do_eval --do_predict  --max_source_length 512 --max_target_length 64 --overwrite_output_dir --save_strategy no  --output_dir out_xsum_200_predict_512 --num_beams 6 --remove_unused_columns true --predict_with_generate  --label_smoothing 0.1 --learning_rate 3e-5 --gradient_accumulation_step 4
python src/run_summarization_custom.py --max_eval_samples 1000 --per_device_eval_batch_size 8  --include_inputs_for_metrics --fp16 --dataset_name xsum   --model_name_or_path facebook/bart-large-xsum  --do_eval  --max_source_length 512 --max_target_length 64 --overwrite_output_dir --save_strategy no  --output_dir out_xsum_1000_eval_512 --num_beams 6 --remove_unused_columns true --predict_with_generate  --label_smoothing 0.1 --learning_rate 3e-5 --gradient_accumulation_step 4

python src/run_summarization_custom.py  --do_eval --per_device_eval_batch_size 32 --fp16 --dataset_name xsum  --preprocessing_num_workers 8   --model_name_or_path facebook/bart-large-xsum  --max_source_length 512 --max_target_length 64 --save_strategy steps  --output_dir out_xsum_eval --num_beams 6 --remove_unused_columns true --predict_with_generate --include_inputs_for_metrics


# PREDICTS
# bart arxiv 8b 512-64 --
#python src/run_summarization_custom.py  --do_predict --per_device_eval_batch_size 8 --fp16 --dataset_name ccdv/arxiv-summarization  --preprocessing_num_workers 8   --model_name_or_path out_arxiv_bart_train_512  --max_source_length 512 --max_target_length 64 --save_strategy steps  --output_dir predict_out_arxiv_bart_train_512_8b --num_beams 6 --remove_unused_columns true --predict_with_generate --include_inputs_for_metrics
#python src/run_qe.py --exp predict_out_arxiv_bart_train_512_8b

# bart arxiv 8b 1024-128
#python src/run_summarization_custom.py  --do_predict --per_device_eval_batch_size 8 --fp16 --dataset_name ccdv/arxiv-summarization  --preprocessing_num_workers 8   --model_name_or_path out_arxiv_bart_large_train_1024_128_b8_beams6  --max_source_length 1024 --max_target_length 128 --save_strategy steps  --output_dir predict_out_arxiv_bart_train_1024_8b --num_beams 6 --remove_unused_columns true --predict_with_generate --include_inputs_for_metrics
#python src/run_qe.py --exp predict_out_arxiv_bart_train_1024_8b

# bart xsum arxiv 8b 512-64 - include CONNECT TOKEN --
#python src/run_summarization_custom.py  --do_predict --per_device_eval_batch_size 8 --fp16 --dataset_name ccdv/arxiv-summarization  --preprocessing_num_workers 8   --model_name_or_path out_arxiv_xsun_pretrain_train_512  --max_source_length 512 --max_target_length 64 --save_strategy steps  --output_dir predict_out_arxiv_xsum_pretrain_train_512_8b --num_beams 6 --remove_unused_columns true --predict_with_generate --include_inputs_for_metrics
#python src/run_qe.py --exp predict_out_arxiv_xsum_pretrain_train_512_8b

# bart xsum arxiv 8b 1024-128 - include CONNECT TOKEN --
#python src/run_summarization_custom.py  --do_predict --per_device_eval_batch_size 8 --fp16 --dataset_name ccdv/arxiv-summarization  --preprocessing_num_workers 8   --model_name_or_path out_arxiv_xsun_pretrain_train_1024_128_b8_beams6  --max_source_length 1024 --max_target_length 128 --save_strategy steps  --output_dir predict_out_arxiv_xsum_pretrain_train_1024_b8 --num_beams 6 --remove_unused_columns true --predict_with_generate --include_inputs_for_metrics
#python src/run_qe.py --exp predict_out_arxiv_xsum_pretrain_train_1024_b8


# bart arxiv arxiv 8b 512-64 - include CONNECT TOKEN 
python src/run_summarization_custom.py  --do_predict --per_device_eval_batch_size 8 --fp16 --dataset_name ccdv/arxiv-summarization  --preprocessing_num_workers 8   --model_name_or_path out_arxiv_pretrainarxiv_train_512_smoothing0  --max_source_length 512 --max_target_length 64 --save_strategy steps  --output_dir predict_out_arxiv_arxiv_pretrain_train_512_8b --num_beams 6 --remove_unused_columns true --predict_with_generate --include_inputs_for_metrics
python src/run_qe.py --exp predict_out_arxiv_arxiv_pretrain_train_512_8b

# bart arxiv arxiv 8b 1024-128 - include CONNECT TOKEN 
python src/run_summarization_custom.py  --do_predict --per_device_eval_batch_size 8 --fp16 --dataset_name ccdv/arxiv-summarization  --preprocessing_num_workers 8   --model_name_or_path out_arxiv_full_arxiv_pretrain_train_1024_128_b8_beams6  --max_source_length 1024 --max_target_length 128 --save_strategy steps  --output_dir predict_out_arxiv_arxiv_pretrain_train_1024_b8 --num_beams 6 --remove_unused_columns true --predict_with_generate --include_inputs_for_metrics
python src/run_qe.py --exp predict_out_arxiv_arxiv_pretrain_train_1024_b8

# bart arxiv 8b 512-128 --
#python src/run_summarization_custom.py  --do_predict --per_device_eval_batch_size 8 --fp16 --dataset_name ccdv/arxiv-summarization  --preprocessing_num_workers 8   --model_name_or_path out_arxiv_bart_train_512_128  --max_source_length 512 --max_target_length 128 --save_strategy steps  --output_dir predict_out_arxiv_bart_train_512_128_8b --num_beams 6 --remove_unused_columns true --predict_with_generate --include_inputs_for_metrics
#python src/run_qe.py --exp predict_out_arxiv_bart_train_512_128_8b
#python src/run_feqa.py --exp predict_out_arxiv_bart_train_512_128_8b

# bart arxiv80000 arxiv 8b 512-128 - include CONNECT TOKEN --F
# python src/run_summarization_custom.py  --do_predict --per_device_eval_batch_size 8 --fp16 --dataset_name ccdv/arxiv-summarization  --preprocessing_num_workers 8   --model_name_or_path out_arxiv_pretrainarxiv_train_512_128  --max_source_length 512 --max_target_length 128 --save_strategy steps  --output_dir predict_out_arxiv80000_arxiv_pretrain_train_512_128_8b --num_beams 6 --remove_unused_columns true --predict_with_generate --include_inputs_for_metrics
#python src/run_qe.py --exp predict_out_arxiv80000_arxiv_pretrain_train_512_128_8b

python src/run_feqa.py --exp predict_out_arxiv80000_arxiv_pretrain_train_512_128_8b
# bart xsum arxiv 8b 512-128 - include CONNECT TOKEN --
#python src/run_summarization_custom.py  --do_predict --per_device_eval_batch_size 8 --fp16 --dataset_name ccdv/arxiv-summarization  --preprocessing_num_workers 8   --model_name_or_path out_arxiv_xsun_pretrain_train_512_128_b8_beams1  --max_source_length 512 --max_target_length 128 --save_strategy steps  --output_dir predict_out_arxiv_xsum_pretrain_train_512_128_8b --num_beams 6 --remove_unused_columns true --predict_with_generate --include_inputs_for_metrics
#python src/run_qe.py --exp predict_out_arxiv_xsum_pretrain_train_512_128_8b
#python src/run_feqa.py --exp predict_out_arxiv_xsum_pretrain_train_512_128_8b

# bart arxiv80000 arxiv 8b 512-64 - include CONNECT TOKEN --
python src/run_summarization_custom.py  --do_predict --per_device_eval_batch_size 8 --fp16 --dataset_name ccdv/arxiv-summarization  --preprocessing_num_workers 8   --model_name_or_path out_arxiv_pretrainarxiv_train_512_smoothing0.1  --max_source_length 512 --max_target_length 64 --save_strategy steps  --output_dir predict_out_arxiv80000_arxiv_pretrain_train_512_8b --num_beams 6 --remove_unused_columns true --predict_with_generate --include_inputs_for_metrics
python src/run_qe.py --exp predict_out_arxiv80000_arxiv_pretrain_train_512_8b

# bart arxiv80000 arxiv 8b 1024-128 - include CONNECT TOKEN --
python src/run_summarization_custom.py  --do_predict --per_device_eval_batch_size 8 --fp16 --dataset_name ccdv/arxiv-summarization  --preprocessing_num_workers 8   --model_name_or_path out_arxiv_arxiv_pretrain_train_1024_128_b8_beams6  --max_source_length 1024 --max_target_length 128 --save_strategy steps  --output_dir predict_out_arxiv80000_arxiv_pretrain_train_1024_b8 --num_beams 6 --remove_unused_columns true --predict_with_generate --include_inputs_for_metrics
python src/run_qe.py --exp predict_out_arxiv80000_arxiv_pretrain_train_1024_b8

# bart xsum 8b 512-64 --
#python src/run_summarization_custom.py  --do_predict --per_device_eval_batch_size 8 --fp16 --dataset_name xsum  --preprocessing_num_workers 8   --model_name_or_path out_xsum_final_bart_train_512  --max_source_length 512 --max_target_length 64 --save_strategy steps  --output_dir predict_out_xsum_bart_train_512_64_8b --num_beams 6 --remove_unused_columns true --predict_with_generate --include_inputs_for_metrics
#python src/run_qe.py --exp predict_out_xsum_bart_train_512_64_8b

# bart xsum xsum 8b 512-64 --
#python src/run_summarization_custom.py  --do_predict --per_device_eval_batch_size 8 --fp16 --dataset_name xsum  --preprocessing_num_workers 8   --model_name_or_path out_xsum_final_pretrain_xsum_bart_train_512  --max_source_length 512 --max_target_length 64 --save_strategy steps  --output_dir predict_out_xsum_xsum_bart_train_512_64_8b --num_beams 6 --remove_unused_columns true --predict_with_generate --include_inputs_for_metrics
#python src/run_qe.py --exp predict_out_xsum_xsum_bart_train_512_64_8b


# bart cnn 8b 512-64 --
#python src/run_summarization_custom.py  --do_predict --per_device_eval_batch_size 8 --fp16 --dataset_name xsum  --preprocessing_num_workers 8   --model_name_or_path out_cnn_final_bart_train_512  --max_source_length 512 --max_target_length 64 --save_strategy steps  --output_dir predict_out_cnn_bart_train_512_64_8b --num_beams 6 --remove_unused_columns true --predict_with_generate --include_inputs_for_metrics
python src/run_qe.py --exp predict_out_cnn_bart_train_512_64_8b

# bart cnn xsum 8b 512-64 --
#python src/run_summarization_custom.py  --do_predict --per_device_eval_batch_size 8 --fp16 --dataset_name xsum  --preprocessing_num_workers 8   --model_name_or_path out_cnn_final_pretrain_xsum_bart_train_512  --max_source_length 512 --max_target_length 64 --save_strategy steps  --output_dir predict_out_cnn_xsum_bart_train_512_64_8b --num_beams 6 --remove_unused_columns true --predict_with_generate --include_inputs_for_metrics
python src/run_qe.py --exp predict_out_cnn_xsum_bart_train_512_64_8b



# evaluations
python src/run_qe.py --exp 
python src/run_feqa.py --exp


set CUDA_VISIBLE_DEVICES=0
set CUDA_VISIBLE_DEVICES=1



